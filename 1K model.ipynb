{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = r\"D:\\GAN\\Newthird\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "latent_dim = 100\n",
    "img_height, img_width, channels = 64, 64, 3\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 40\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*channels, [0.5]*channels)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom dataset loader for unlabeled images\n",
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(root_dir, file) for file in os.listdir(root_dir) if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Dummy label\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = UnlabeledImageDataset(r\"D:\\Third hand\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = img_height // 16\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 512 * self.init_size ** 2))\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 512, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Function to save generated images\n",
    "def save_generated_images(epoch, generator, latent_dim, output_dir):\n",
    "    noise = torch.randn(16, latent_dim, device=device)\n",
    "    generated_images = generator(noise).detach().cpu()\n",
    "    generated_images = (generated_images + 1) / 2  # Rescale images to [0, 1]\n",
    "    save_image(generated_images, os.path.join(output_dir, f\"gan_generated_image_epoch_{epoch}.png\"), nrow=4, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/40] [Batch 0/38] [D loss: 0.7071085572242737] [G loss: 0.6984012126922607]\n",
      "[Epoch 0/40] [Batch 1/38] [D loss: 0.5542237162590027] [G loss: 0.7601761221885681]\n",
      "[Epoch 0/40] [Batch 2/38] [D loss: 0.4746251106262207] [G loss: 0.8660228848457336]\n",
      "[Epoch 0/40] [Batch 3/38] [D loss: 0.4085870087146759] [G loss: 0.9964646100997925]\n",
      "[Epoch 0/40] [Batch 4/38] [D loss: 0.34300774335861206] [G loss: 1.1165204048156738]\n",
      "[Epoch 0/40] [Batch 5/38] [D loss: 0.2737886905670166] [G loss: 1.4092862606048584]\n",
      "[Epoch 0/40] [Batch 6/38] [D loss: 0.32722747325897217] [G loss: 1.3804874420166016]\n",
      "[Epoch 0/40] [Batch 7/38] [D loss: 0.2520400285720825] [G loss: 1.3890471458435059]\n",
      "[Epoch 0/40] [Batch 8/38] [D loss: 0.35537856817245483] [G loss: 1.563973069190979]\n",
      "[Epoch 0/40] [Batch 9/38] [D loss: 0.3563348650932312] [G loss: 1.3734259605407715]\n",
      "[Epoch 0/40] [Batch 10/38] [D loss: 0.34665393829345703] [G loss: 1.5800652503967285]\n",
      "[Epoch 0/40] [Batch 11/38] [D loss: 0.41131848096847534] [G loss: 1.4583431482315063]\n",
      "[Epoch 0/40] [Batch 12/38] [D loss: 0.437111496925354] [G loss: 1.6496620178222656]\n",
      "[Epoch 0/40] [Batch 13/38] [D loss: 0.35687264800071716] [G loss: 1.8213090896606445]\n",
      "[Epoch 0/40] [Batch 14/38] [D loss: 0.3379303812980652] [G loss: 1.6084821224212646]\n",
      "[Epoch 0/40] [Batch 15/38] [D loss: 0.42201322317123413] [G loss: 1.4520015716552734]\n",
      "[Epoch 0/40] [Batch 16/38] [D loss: 0.3269253075122833] [G loss: 1.2590261697769165]\n",
      "[Epoch 0/40] [Batch 17/38] [D loss: 0.26842838525772095] [G loss: 1.8174835443496704]\n",
      "[Epoch 0/40] [Batch 18/38] [D loss: 0.3737170100212097] [G loss: 1.626509428024292]\n",
      "[Epoch 0/40] [Batch 19/38] [D loss: 0.3681092858314514] [G loss: 1.235448956489563]\n",
      "[Epoch 0/40] [Batch 20/38] [D loss: 0.37711191177368164] [G loss: 1.5643959045410156]\n",
      "[Epoch 0/40] [Batch 21/38] [D loss: 0.45433667302131653] [G loss: 0.9779692888259888]\n",
      "[Epoch 0/40] [Batch 22/38] [D loss: 0.39212673902511597] [G loss: 1.5218394994735718]\n",
      "[Epoch 0/40] [Batch 23/38] [D loss: 0.7098565697669983] [G loss: 1.1681557893753052]\n",
      "[Epoch 0/40] [Batch 24/38] [D loss: 0.579179048538208] [G loss: 0.8333979249000549]\n",
      "[Epoch 0/40] [Batch 25/38] [D loss: 0.48683372139930725] [G loss: 1.5667728185653687]\n",
      "[Epoch 0/40] [Batch 26/38] [D loss: 0.4109538793563843] [G loss: 1.6667091846466064]\n",
      "[Epoch 0/40] [Batch 27/38] [D loss: 0.42337000370025635] [G loss: 1.2600996494293213]\n",
      "[Epoch 0/40] [Batch 28/38] [D loss: 0.5806872844696045] [G loss: 1.1038000583648682]\n",
      "[Epoch 0/40] [Batch 29/38] [D loss: 0.6486060619354248] [G loss: 1.10635507106781]\n",
      "[Epoch 0/40] [Batch 30/38] [D loss: 0.5919344425201416] [G loss: 1.2485756874084473]\n",
      "[Epoch 0/40] [Batch 31/38] [D loss: 0.5281617641448975] [G loss: 1.35440993309021]\n",
      "[Epoch 0/40] [Batch 32/38] [D loss: 0.43059372901916504] [G loss: 1.667647361755371]\n",
      "[Epoch 0/40] [Batch 33/38] [D loss: 0.44465911388397217] [G loss: 1.4479154348373413]\n",
      "[Epoch 0/40] [Batch 34/38] [D loss: 0.4008052349090576] [G loss: 1.5036311149597168]\n",
      "[Epoch 0/40] [Batch 35/38] [D loss: 0.2958887219429016] [G loss: 1.6423200368881226]\n",
      "[Epoch 0/40] [Batch 36/38] [D loss: 0.5201011300086975] [G loss: 1.7831628322601318]\n",
      "[Epoch 0/40] [Batch 37/38] [D loss: 0.3685937523841858] [G loss: 1.0500848293304443]\n",
      "[Epoch 1/40] [Batch 0/38] [D loss: 0.6417792439460754] [G loss: 1.1480509042739868]\n",
      "[Epoch 1/40] [Batch 1/38] [D loss: 0.45961084961891174] [G loss: 1.7091608047485352]\n",
      "[Epoch 1/40] [Batch 2/38] [D loss: 0.3366242051124573] [G loss: 1.8102971315383911]\n",
      "[Epoch 1/40] [Batch 3/38] [D loss: 0.4611698389053345] [G loss: 1.3327968120574951]\n",
      "[Epoch 1/40] [Batch 4/38] [D loss: 0.270198255777359] [G loss: 1.3174114227294922]\n",
      "[Epoch 1/40] [Batch 5/38] [D loss: 0.35500389337539673] [G loss: 1.8995312452316284]\n",
      "[Epoch 1/40] [Batch 6/38] [D loss: 0.4028868079185486] [G loss: 1.2343562841415405]\n",
      "[Epoch 1/40] [Batch 7/38] [D loss: 0.4207608699798584] [G loss: 1.6471178531646729]\n",
      "[Epoch 1/40] [Batch 8/38] [D loss: 0.5132302045822144] [G loss: 1.3282194137573242]\n",
      "[Epoch 1/40] [Batch 9/38] [D loss: 0.4313056766986847] [G loss: 1.4568634033203125]\n",
      "[Epoch 1/40] [Batch 10/38] [D loss: 0.5195256471633911] [G loss: 1.1343244314193726]\n",
      "[Epoch 1/40] [Batch 11/38] [D loss: 0.5093480944633484] [G loss: 1.3796882629394531]\n",
      "[Epoch 1/40] [Batch 12/38] [D loss: 0.47298064827919006] [G loss: 1.024993658065796]\n",
      "[Epoch 1/40] [Batch 13/38] [D loss: 0.5358801484107971] [G loss: 1.9347641468048096]\n",
      "[Epoch 1/40] [Batch 14/38] [D loss: 0.40819209814071655] [G loss: 1.2398903369903564]\n",
      "[Epoch 1/40] [Batch 15/38] [D loss: 0.4614754915237427] [G loss: 1.0448338985443115]\n",
      "[Epoch 1/40] [Batch 16/38] [D loss: 0.4787178635597229] [G loss: 1.7356330156326294]\n",
      "[Epoch 1/40] [Batch 17/38] [D loss: 0.47110992670059204] [G loss: 1.2639765739440918]\n",
      "[Epoch 1/40] [Batch 18/38] [D loss: 0.41888627409935] [G loss: 1.5454131364822388]\n",
      "[Epoch 1/40] [Batch 19/38] [D loss: 0.3890393376350403] [G loss: 1.662227988243103]\n",
      "[Epoch 1/40] [Batch 20/38] [D loss: 0.4482874870300293] [G loss: 1.735517144203186]\n",
      "[Epoch 1/40] [Batch 21/38] [D loss: 0.42394953966140747] [G loss: 1.3224607706069946]\n",
      "[Epoch 1/40] [Batch 22/38] [D loss: 0.32724320888519287] [G loss: 1.439081072807312]\n",
      "[Epoch 1/40] [Batch 23/38] [D loss: 0.46086016297340393] [G loss: 1.6555325984954834]\n",
      "[Epoch 1/40] [Batch 24/38] [D loss: 0.483177125453949] [G loss: 1.1929588317871094]\n",
      "[Epoch 1/40] [Batch 25/38] [D loss: 0.34691745042800903] [G loss: 1.5870965719223022]\n",
      "[Epoch 1/40] [Batch 26/38] [D loss: 0.4024692177772522] [G loss: 1.5997488498687744]\n",
      "[Epoch 1/40] [Batch 27/38] [D loss: 0.3655592203140259] [G loss: 1.5813130140304565]\n",
      "[Epoch 1/40] [Batch 28/38] [D loss: 0.6332359313964844] [G loss: 1.5842792987823486]\n",
      "[Epoch 1/40] [Batch 29/38] [D loss: 0.5747756958007812] [G loss: 1.0901302099227905]\n",
      "[Epoch 1/40] [Batch 30/38] [D loss: 0.4005942642688751] [G loss: 1.7105255126953125]\n",
      "[Epoch 1/40] [Batch 31/38] [D loss: 0.32617005705833435] [G loss: 1.8670403957366943]\n",
      "[Epoch 1/40] [Batch 32/38] [D loss: 0.3355148136615753] [G loss: 1.4754830598831177]\n",
      "[Epoch 1/40] [Batch 33/38] [D loss: 0.5179811716079712] [G loss: 1.138056755065918]\n",
      "[Epoch 1/40] [Batch 34/38] [D loss: 0.41694700717926025] [G loss: 1.6352566480636597]\n",
      "[Epoch 1/40] [Batch 35/38] [D loss: 0.7348263263702393] [G loss: 1.127328634262085]\n",
      "[Epoch 1/40] [Batch 36/38] [D loss: 0.547703742980957] [G loss: 1.349467158317566]\n",
      "[Epoch 1/40] [Batch 37/38] [D loss: 0.4939720630645752] [G loss: 1.0142940282821655]\n",
      "[Epoch 2/40] [Batch 0/38] [D loss: 0.4075595736503601] [G loss: 1.6408709287643433]\n",
      "[Epoch 2/40] [Batch 1/38] [D loss: 0.41830533742904663] [G loss: 1.5910730361938477]\n",
      "[Epoch 2/40] [Batch 2/38] [D loss: 0.3467683792114258] [G loss: 1.5614583492279053]\n",
      "[Epoch 2/40] [Batch 3/38] [D loss: 0.4132589101791382] [G loss: 1.403825283050537]\n",
      "[Epoch 2/40] [Batch 4/38] [D loss: 0.4526565968990326] [G loss: 1.2484573125839233]\n",
      "[Epoch 2/40] [Batch 5/38] [D loss: 0.3644999861717224] [G loss: 1.6360135078430176]\n",
      "[Epoch 2/40] [Batch 6/38] [D loss: 0.39833831787109375] [G loss: 1.3767489194869995]\n",
      "[Epoch 2/40] [Batch 7/38] [D loss: 0.5107980966567993] [G loss: 1.459373950958252]\n",
      "[Epoch 2/40] [Batch 8/38] [D loss: 0.4118426442146301] [G loss: 1.226360559463501]\n",
      "[Epoch 2/40] [Batch 9/38] [D loss: 0.28130191564559937] [G loss: 2.157642126083374]\n",
      "[Epoch 2/40] [Batch 10/38] [D loss: 0.36487433314323425] [G loss: 1.822123646736145]\n",
      "[Epoch 2/40] [Batch 11/38] [D loss: 0.3458296060562134] [G loss: 1.7912887334823608]\n",
      "[Epoch 2/40] [Batch 12/38] [D loss: 0.45691972970962524] [G loss: 1.806623935699463]\n",
      "[Epoch 2/40] [Batch 13/38] [D loss: 0.362948477268219] [G loss: 1.5736541748046875]\n",
      "[Epoch 2/40] [Batch 14/38] [D loss: 0.582970917224884] [G loss: 1.5784521102905273]\n",
      "[Epoch 2/40] [Batch 15/38] [D loss: 0.392233669757843] [G loss: 1.9414613246917725]\n",
      "[Epoch 2/40] [Batch 16/38] [D loss: 0.370771586894989] [G loss: 1.8648810386657715]\n",
      "[Epoch 2/40] [Batch 17/38] [D loss: 0.2495116889476776] [G loss: 2.1149468421936035]\n",
      "[Epoch 2/40] [Batch 18/38] [D loss: 0.23418700695037842] [G loss: 1.770982027053833]\n",
      "[Epoch 2/40] [Batch 19/38] [D loss: 0.27379685640335083] [G loss: 1.725042462348938]\n",
      "[Epoch 2/40] [Batch 20/38] [D loss: 0.2452467828989029] [G loss: 1.5244381427764893]\n",
      "[Epoch 2/40] [Batch 21/38] [D loss: 0.21123647689819336] [G loss: 1.83171546459198]\n",
      "[Epoch 2/40] [Batch 22/38] [D loss: 0.21319574117660522] [G loss: 2.004603385925293]\n",
      "[Epoch 2/40] [Batch 23/38] [D loss: 0.3014858365058899] [G loss: 1.8825151920318604]\n",
      "[Epoch 2/40] [Batch 24/38] [D loss: 0.40882179141044617] [G loss: 0.9614590406417847]\n",
      "[Epoch 2/40] [Batch 25/38] [D loss: 0.49540361762046814] [G loss: 1.3265818357467651]\n",
      "[Epoch 2/40] [Batch 26/38] [D loss: 0.5274360179901123] [G loss: 1.5357367992401123]\n",
      "[Epoch 2/40] [Batch 27/38] [D loss: 0.6876726150512695] [G loss: 1.059816598892212]\n",
      "[Epoch 2/40] [Batch 28/38] [D loss: 0.40283480286598206] [G loss: 1.3741339445114136]\n",
      "[Epoch 2/40] [Batch 29/38] [D loss: 0.6548652052879333] [G loss: 1.7364650964736938]\n",
      "[Epoch 2/40] [Batch 30/38] [D loss: 0.42699235677719116] [G loss: 1.0706405639648438]\n",
      "[Epoch 2/40] [Batch 31/38] [D loss: 0.524383008480072] [G loss: 1.7496907711029053]\n",
      "[Epoch 2/40] [Batch 32/38] [D loss: 0.3362727165222168] [G loss: 1.5381250381469727]\n",
      "[Epoch 2/40] [Batch 33/38] [D loss: 0.461381196975708] [G loss: 1.3458895683288574]\n",
      "[Epoch 2/40] [Batch 34/38] [D loss: 0.4814617931842804] [G loss: 1.5971487760543823]\n",
      "[Epoch 2/40] [Batch 35/38] [D loss: 0.32006019353866577] [G loss: 1.517391562461853]\n",
      "[Epoch 2/40] [Batch 36/38] [D loss: 0.44209980964660645] [G loss: 2.0222320556640625]\n",
      "[Epoch 2/40] [Batch 37/38] [D loss: 0.49984562397003174] [G loss: 1.4487617015838623]\n",
      "[Epoch 3/40] [Batch 0/38] [D loss: 0.60944664478302] [G loss: 0.7961171865463257]\n",
      "[Epoch 3/40] [Batch 1/38] [D loss: 0.5466926693916321] [G loss: 2.697449207305908]\n",
      "[Epoch 3/40] [Batch 2/38] [D loss: 0.6954532861709595] [G loss: 1.551666498184204]\n",
      "[Epoch 3/40] [Batch 3/38] [D loss: 0.6858075261116028] [G loss: 0.5304731726646423]\n",
      "[Epoch 3/40] [Batch 4/38] [D loss: 0.4960513710975647] [G loss: 1.7715132236480713]\n",
      "[Epoch 3/40] [Batch 5/38] [D loss: 0.47069883346557617] [G loss: 1.5646076202392578]\n",
      "[Epoch 3/40] [Batch 6/38] [D loss: 0.3585365116596222] [G loss: 1.041933536529541]\n",
      "[Epoch 3/40] [Batch 7/38] [D loss: 0.4551509618759155] [G loss: 1.7186201810836792]\n",
      "[Epoch 3/40] [Batch 8/38] [D loss: 0.5539640784263611] [G loss: 1.1229028701782227]\n",
      "[Epoch 3/40] [Batch 9/38] [D loss: 0.5810062289237976] [G loss: 1.3938792943954468]\n",
      "[Epoch 3/40] [Batch 10/38] [D loss: 0.5130497217178345] [G loss: 0.7970027923583984]\n",
      "[Epoch 3/40] [Batch 11/38] [D loss: 0.3116569519042969] [G loss: 1.591709852218628]\n",
      "[Epoch 3/40] [Batch 12/38] [D loss: 0.4022020697593689] [G loss: 2.0690011978149414]\n",
      "[Epoch 3/40] [Batch 13/38] [D loss: 0.4849582612514496] [G loss: 1.0817835330963135]\n",
      "[Epoch 3/40] [Batch 14/38] [D loss: 0.3955109715461731] [G loss: 1.553882122039795]\n",
      "[Epoch 3/40] [Batch 15/38] [D loss: 0.4096606969833374] [G loss: 1.284641981124878]\n",
      "[Epoch 3/40] [Batch 16/38] [D loss: 0.36327052116394043] [G loss: 1.294062614440918]\n",
      "[Epoch 3/40] [Batch 17/38] [D loss: 0.4480487108230591] [G loss: 1.4849004745483398]\n",
      "[Epoch 3/40] [Batch 18/38] [D loss: 0.44968074560165405] [G loss: 1.469366192817688]\n",
      "[Epoch 3/40] [Batch 19/38] [D loss: 0.5410799384117126] [G loss: 1.0512158870697021]\n",
      "[Epoch 3/40] [Batch 20/38] [D loss: 0.6061643362045288] [G loss: 1.2811957597732544]\n",
      "[Epoch 3/40] [Batch 21/38] [D loss: 0.4440564513206482] [G loss: 1.157224178314209]\n",
      "[Epoch 3/40] [Batch 22/38] [D loss: 0.6261969208717346] [G loss: 1.2558128833770752]\n",
      "[Epoch 3/40] [Batch 23/38] [D loss: 0.40501922369003296] [G loss: 1.013687014579773]\n",
      "[Epoch 3/40] [Batch 24/38] [D loss: 0.41806983947753906] [G loss: 1.2373992204666138]\n",
      "[Epoch 3/40] [Batch 25/38] [D loss: 0.32522499561309814] [G loss: 1.747342586517334]\n",
      "[Epoch 3/40] [Batch 26/38] [D loss: 0.30391496419906616] [G loss: 1.6597566604614258]\n",
      "[Epoch 3/40] [Batch 27/38] [D loss: 0.3100191354751587] [G loss: 1.7566105127334595]\n",
      "[Epoch 3/40] [Batch 28/38] [D loss: 0.28894302248954773] [G loss: 1.4144716262817383]\n",
      "[Epoch 3/40] [Batch 29/38] [D loss: 0.48290708661079407] [G loss: 1.560773253440857]\n",
      "[Epoch 3/40] [Batch 30/38] [D loss: 0.5425873398780823] [G loss: 1.054430603981018]\n",
      "[Epoch 3/40] [Batch 31/38] [D loss: 0.5287349224090576] [G loss: 1.3196470737457275]\n",
      "[Epoch 3/40] [Batch 32/38] [D loss: 0.7505464553833008] [G loss: 1.286376953125]\n",
      "[Epoch 3/40] [Batch 33/38] [D loss: 0.7867304086685181] [G loss: 0.40943291783332825]\n",
      "[Epoch 3/40] [Batch 34/38] [D loss: 0.8902158141136169] [G loss: 2.256603240966797]\n",
      "[Epoch 3/40] [Batch 35/38] [D loss: 0.4572422504425049] [G loss: 1.0406323671340942]\n",
      "[Epoch 3/40] [Batch 36/38] [D loss: 0.5769410133361816] [G loss: 0.8366891145706177]\n",
      "[Epoch 3/40] [Batch 37/38] [D loss: 0.5927137136459351] [G loss: 1.6263500452041626]\n",
      "[Epoch 4/40] [Batch 0/38] [D loss: 0.4302123785018921] [G loss: 1.2490026950836182]\n",
      "[Epoch 4/40] [Batch 1/38] [D loss: 0.3878478407859802] [G loss: 1.1061686277389526]\n",
      "[Epoch 4/40] [Batch 2/38] [D loss: 0.4564257860183716] [G loss: 1.3314164876937866]\n",
      "[Epoch 4/40] [Batch 3/38] [D loss: 0.41349372267723083] [G loss: 1.0146076679229736]\n",
      "[Epoch 4/40] [Batch 4/38] [D loss: 0.4414074122905731] [G loss: 1.7567614316940308]\n",
      "[Epoch 4/40] [Batch 5/38] [D loss: 0.3933444023132324] [G loss: 1.73940908908844]\n",
      "[Epoch 4/40] [Batch 6/38] [D loss: 0.45587077736854553] [G loss: 0.9302268028259277]\n",
      "[Epoch 4/40] [Batch 7/38] [D loss: 0.4633429944515228] [G loss: 1.1873785257339478]\n",
      "[Epoch 4/40] [Batch 8/38] [D loss: 0.33471783995628357] [G loss: 1.96855628490448]\n",
      "[Epoch 4/40] [Batch 9/38] [D loss: 0.4515056014060974] [G loss: 1.2346200942993164]\n",
      "[Epoch 4/40] [Batch 10/38] [D loss: 0.5674787163734436] [G loss: 1.0556143522262573]\n",
      "[Epoch 4/40] [Batch 11/38] [D loss: 0.5489270091056824] [G loss: 1.1544924974441528]\n",
      "[Epoch 4/40] [Batch 12/38] [D loss: 0.41573986411094666] [G loss: 1.6553603410720825]\n",
      "[Epoch 4/40] [Batch 13/38] [D loss: 0.4057955741882324] [G loss: 1.189197063446045]\n",
      "[Epoch 4/40] [Batch 14/38] [D loss: 0.3950057327747345] [G loss: 1.3849570751190186]\n",
      "[Epoch 4/40] [Batch 15/38] [D loss: 0.4603191018104553] [G loss: 1.2396042346954346]\n",
      "[Epoch 4/40] [Batch 16/38] [D loss: 0.4352967441082001] [G loss: 1.7754297256469727]\n",
      "[Epoch 4/40] [Batch 17/38] [D loss: 0.4834507405757904] [G loss: 1.0634857416152954]\n",
      "[Epoch 4/40] [Batch 18/38] [D loss: 0.5485523343086243] [G loss: 1.2960119247436523]\n",
      "[Epoch 4/40] [Batch 19/38] [D loss: 0.39978206157684326] [G loss: 1.5601474046707153]\n",
      "[Epoch 4/40] [Batch 20/38] [D loss: 0.3507075607776642] [G loss: 1.5783374309539795]\n",
      "[Epoch 4/40] [Batch 21/38] [D loss: 0.49149268865585327] [G loss: 1.1351882219314575]\n",
      "[Epoch 4/40] [Batch 22/38] [D loss: 0.4918082356452942] [G loss: 1.2381541728973389]\n",
      "[Epoch 4/40] [Batch 23/38] [D loss: 0.5288854837417603] [G loss: 1.2769334316253662]\n",
      "[Epoch 4/40] [Batch 24/38] [D loss: 0.49394696950912476] [G loss: 1.0050462484359741]\n",
      "[Epoch 4/40] [Batch 25/38] [D loss: 0.44545891880989075] [G loss: 1.371424913406372]\n",
      "[Epoch 4/40] [Batch 26/38] [D loss: 0.3814893364906311] [G loss: 1.4003074169158936]\n",
      "[Epoch 4/40] [Batch 27/38] [D loss: 0.5192152857780457] [G loss: 1.226754069328308]\n",
      "[Epoch 4/40] [Batch 28/38] [D loss: 0.5341857075691223] [G loss: 0.9591182470321655]\n",
      "[Epoch 4/40] [Batch 29/38] [D loss: 0.38389894366264343] [G loss: 1.7539854049682617]\n",
      "[Epoch 4/40] [Batch 30/38] [D loss: 0.47522249817848206] [G loss: 1.2463877201080322]\n",
      "[Epoch 4/40] [Batch 31/38] [D loss: 0.3638313114643097] [G loss: 1.3703792095184326]\n",
      "[Epoch 4/40] [Batch 32/38] [D loss: 0.3284786343574524] [G loss: 1.6805521249771118]\n",
      "[Epoch 4/40] [Batch 33/38] [D loss: 0.33288276195526123] [G loss: 1.7933577299118042]\n",
      "[Epoch 4/40] [Batch 34/38] [D loss: 0.30458831787109375] [G loss: 1.643080234527588]\n",
      "[Epoch 4/40] [Batch 35/38] [D loss: 0.32492750883102417] [G loss: 1.3964203596115112]\n",
      "[Epoch 4/40] [Batch 36/38] [D loss: 0.35411888360977173] [G loss: 1.295621395111084]\n",
      "[Epoch 4/40] [Batch 37/38] [D loss: 0.48582983016967773] [G loss: 1.491494059562683]\n",
      "[Epoch 5/40] [Batch 0/38] [D loss: 0.41757524013519287] [G loss: 1.5040720701217651]\n",
      "[Epoch 5/40] [Batch 1/38] [D loss: 0.5805020332336426] [G loss: 0.8414356112480164]\n",
      "[Epoch 5/40] [Batch 2/38] [D loss: 0.49086862802505493] [G loss: 1.5728285312652588]\n",
      "[Epoch 5/40] [Batch 3/38] [D loss: 0.6832982301712036] [G loss: 1.1120356321334839]\n",
      "[Epoch 5/40] [Batch 4/38] [D loss: 0.49478697776794434] [G loss: 1.029962182044983]\n",
      "[Epoch 5/40] [Batch 5/38] [D loss: 0.3756207823753357] [G loss: 1.1304986476898193]\n",
      "[Epoch 5/40] [Batch 6/38] [D loss: 0.576040506362915] [G loss: 1.3666223287582397]\n",
      "[Epoch 5/40] [Batch 7/38] [D loss: 0.4342293441295624] [G loss: 1.28005051612854]\n",
      "[Epoch 5/40] [Batch 8/38] [D loss: 0.36541080474853516] [G loss: 1.4073647260665894]\n",
      "[Epoch 5/40] [Batch 9/38] [D loss: 0.4985883831977844] [G loss: 1.2972030639648438]\n",
      "[Epoch 5/40] [Batch 10/38] [D loss: 0.4606655538082123] [G loss: 1.6918858289718628]\n",
      "[Epoch 5/40] [Batch 11/38] [D loss: 0.40616607666015625] [G loss: 1.5925426483154297]\n",
      "[Epoch 5/40] [Batch 12/38] [D loss: 0.4204639792442322] [G loss: 1.109838604927063]\n",
      "[Epoch 5/40] [Batch 13/38] [D loss: 0.6167044639587402] [G loss: 1.6057751178741455]\n",
      "[Epoch 5/40] [Batch 14/38] [D loss: 0.487834095954895] [G loss: 1.2957117557525635]\n",
      "[Epoch 5/40] [Batch 15/38] [D loss: 0.3993947505950928] [G loss: 1.3695932626724243]\n",
      "[Epoch 5/40] [Batch 16/38] [D loss: 0.434390664100647] [G loss: 1.3696575164794922]\n",
      "[Epoch 5/40] [Batch 17/38] [D loss: 0.4537191390991211] [G loss: 1.588557243347168]\n",
      "[Epoch 5/40] [Batch 18/38] [D loss: 0.6370865702629089] [G loss: 0.9659063816070557]\n",
      "[Epoch 5/40] [Batch 19/38] [D loss: 0.44606274366378784] [G loss: 1.0438473224639893]\n",
      "[Epoch 5/40] [Batch 20/38] [D loss: 0.34231558442115784] [G loss: 1.9649808406829834]\n",
      "[Epoch 5/40] [Batch 21/38] [D loss: 0.46016883850097656] [G loss: 1.4188363552093506]\n",
      "[Epoch 5/40] [Batch 22/38] [D loss: 0.47280383110046387] [G loss: 1.0672199726104736]\n",
      "[Epoch 5/40] [Batch 23/38] [D loss: 0.473758339881897] [G loss: 2.492976665496826]\n",
      "[Epoch 5/40] [Batch 24/38] [D loss: 0.4191873371601105] [G loss: 1.4210208654403687]\n",
      "[Epoch 5/40] [Batch 25/38] [D loss: 0.47227680683135986] [G loss: 0.9371674060821533]\n",
      "[Epoch 5/40] [Batch 26/38] [D loss: 0.32832199335098267] [G loss: 1.9105507135391235]\n",
      "[Epoch 5/40] [Batch 27/38] [D loss: 0.39469417929649353] [G loss: 1.8955798149108887]\n",
      "[Epoch 5/40] [Batch 28/38] [D loss: 0.4518345296382904] [G loss: 1.079514741897583]\n",
      "[Epoch 5/40] [Batch 29/38] [D loss: 0.44668012857437134] [G loss: 1.3391408920288086]\n",
      "[Epoch 5/40] [Batch 30/38] [D loss: 0.476402223110199] [G loss: 1.7216781377792358]\n",
      "[Epoch 5/40] [Batch 31/38] [D loss: 0.43579280376434326] [G loss: 1.4824165105819702]\n",
      "[Epoch 5/40] [Batch 32/38] [D loss: 0.4239249527454376] [G loss: 1.3032019138336182]\n",
      "[Epoch 5/40] [Batch 33/38] [D loss: 0.46389609575271606] [G loss: 1.8275988101959229]\n",
      "[Epoch 5/40] [Batch 34/38] [D loss: 0.43633878231048584] [G loss: 1.4624329805374146]\n",
      "[Epoch 5/40] [Batch 35/38] [D loss: 0.5195728540420532] [G loss: 1.0684937238693237]\n",
      "[Epoch 5/40] [Batch 36/38] [D loss: 0.4293275475502014] [G loss: 1.3868803977966309]\n",
      "[Epoch 5/40] [Batch 37/38] [D loss: 0.7871218919754028] [G loss: 1.1975656747817993]\n",
      "[Epoch 6/40] [Batch 0/38] [D loss: 0.5176355838775635] [G loss: 1.3183923959732056]\n",
      "[Epoch 6/40] [Batch 1/38] [D loss: 0.5823493003845215] [G loss: 1.976723074913025]\n",
      "[Epoch 6/40] [Batch 2/38] [D loss: 0.5281575918197632] [G loss: 1.1157488822937012]\n",
      "[Epoch 6/40] [Batch 3/38] [D loss: 0.5729609727859497] [G loss: 1.1124292612075806]\n",
      "[Epoch 6/40] [Batch 4/38] [D loss: 0.4767324924468994] [G loss: 1.1275607347488403]\n",
      "[Epoch 6/40] [Batch 5/38] [D loss: 0.5447661280632019] [G loss: 1.3972822427749634]\n",
      "[Epoch 6/40] [Batch 6/38] [D loss: 0.5250380635261536] [G loss: 1.6207869052886963]\n",
      "[Epoch 6/40] [Batch 7/38] [D loss: 0.46906301379203796] [G loss: 1.6538581848144531]\n",
      "[Epoch 6/40] [Batch 8/38] [D loss: 0.4179585576057434] [G loss: 1.0346564054489136]\n",
      "[Epoch 6/40] [Batch 9/38] [D loss: 0.35429108142852783] [G loss: 2.2592387199401855]\n",
      "[Epoch 6/40] [Batch 10/38] [D loss: 0.47881391644477844] [G loss: 2.307910680770874]\n",
      "[Epoch 6/40] [Batch 11/38] [D loss: 0.5810931921005249] [G loss: 0.6511936783790588]\n",
      "[Epoch 6/40] [Batch 12/38] [D loss: 0.47768768668174744] [G loss: 1.7281780242919922]\n",
      "[Epoch 6/40] [Batch 13/38] [D loss: 0.6289225816726685] [G loss: 1.27629816532135]\n",
      "[Epoch 6/40] [Batch 14/38] [D loss: 0.5596882104873657] [G loss: 0.9208059310913086]\n",
      "[Epoch 6/40] [Batch 15/38] [D loss: 0.5283712148666382] [G loss: 1.339400053024292]\n",
      "[Epoch 6/40] [Batch 16/38] [D loss: 0.6669335961341858] [G loss: 1.2070763111114502]\n",
      "[Epoch 6/40] [Batch 17/38] [D loss: 0.567962646484375] [G loss: 0.9455937147140503]\n",
      "[Epoch 6/40] [Batch 18/38] [D loss: 0.5100599527359009] [G loss: 0.9079831838607788]\n",
      "[Epoch 6/40] [Batch 19/38] [D loss: 0.44312727451324463] [G loss: 1.4078350067138672]\n",
      "[Epoch 6/40] [Batch 20/38] [D loss: 0.36528682708740234] [G loss: 1.2095791101455688]\n",
      "[Epoch 6/40] [Batch 21/38] [D loss: 0.5740379095077515] [G loss: 1.3424385786056519]\n",
      "[Epoch 6/40] [Batch 22/38] [D loss: 0.3913896381855011] [G loss: 1.1099836826324463]\n",
      "[Epoch 6/40] [Batch 23/38] [D loss: 0.39680927991867065] [G loss: 1.3758962154388428]\n",
      "[Epoch 6/40] [Batch 24/38] [D loss: 0.3874148726463318] [G loss: 1.2640581130981445]\n",
      "[Epoch 6/40] [Batch 25/38] [D loss: 0.5198038816452026] [G loss: 1.5232079029083252]\n",
      "[Epoch 6/40] [Batch 26/38] [D loss: 0.4081672430038452] [G loss: 1.3081021308898926]\n",
      "[Epoch 6/40] [Batch 27/38] [D loss: 0.5489146113395691] [G loss: 1.0918339490890503]\n",
      "[Epoch 6/40] [Batch 28/38] [D loss: 0.4377470016479492] [G loss: 1.4230161905288696]\n",
      "[Epoch 6/40] [Batch 29/38] [D loss: 0.4065626859664917] [G loss: 1.5431135892868042]\n",
      "[Epoch 6/40] [Batch 30/38] [D loss: 0.40064334869384766] [G loss: 1.2414493560791016]\n",
      "[Epoch 6/40] [Batch 31/38] [D loss: 0.48647254705429077] [G loss: 1.9314732551574707]\n",
      "[Epoch 6/40] [Batch 32/38] [D loss: 0.5697757005691528] [G loss: 1.3183889389038086]\n",
      "[Epoch 6/40] [Batch 33/38] [D loss: 0.5341031551361084] [G loss: 1.381516695022583]\n",
      "[Epoch 6/40] [Batch 34/38] [D loss: 0.6964068412780762] [G loss: 1.24712073802948]\n",
      "[Epoch 6/40] [Batch 35/38] [D loss: 0.6438986659049988] [G loss: 1.2454502582550049]\n",
      "[Epoch 6/40] [Batch 36/38] [D loss: 0.41891855001449585] [G loss: 1.5375193357467651]\n",
      "[Epoch 6/40] [Batch 37/38] [D loss: 0.48705294728279114] [G loss: 1.6658046245574951]\n",
      "[Epoch 7/40] [Batch 0/38] [D loss: 0.40002021193504333] [G loss: 1.3440316915512085]\n",
      "[Epoch 7/40] [Batch 1/38] [D loss: 0.363071471452713] [G loss: 1.88376784324646]\n",
      "[Epoch 7/40] [Batch 2/38] [D loss: 0.5180035829544067] [G loss: 1.3891066312789917]\n",
      "[Epoch 7/40] [Batch 3/38] [D loss: 0.48483705520629883] [G loss: 1.5938457250595093]\n",
      "[Epoch 7/40] [Batch 4/38] [D loss: 0.6139879822731018] [G loss: 0.684664249420166]\n",
      "[Epoch 7/40] [Batch 5/38] [D loss: 0.6402833461761475] [G loss: 1.598630666732788]\n",
      "[Epoch 7/40] [Batch 6/38] [D loss: 0.6849453449249268] [G loss: 0.8775913119316101]\n",
      "[Epoch 7/40] [Batch 7/38] [D loss: 0.6408818960189819] [G loss: 1.2151687145233154]\n",
      "[Epoch 7/40] [Batch 8/38] [D loss: 0.4420686662197113] [G loss: 1.7183489799499512]\n",
      "[Epoch 7/40] [Batch 9/38] [D loss: 0.55766761302948] [G loss: 0.9098528623580933]\n",
      "[Epoch 7/40] [Batch 10/38] [D loss: 0.5280110239982605] [G loss: 1.3919851779937744]\n",
      "[Epoch 7/40] [Batch 11/38] [D loss: 0.6324067711830139] [G loss: 0.9780320525169373]\n",
      "[Epoch 7/40] [Batch 12/38] [D loss: 0.5955665707588196] [G loss: 0.8886538147926331]\n",
      "[Epoch 7/40] [Batch 13/38] [D loss: 0.387492299079895] [G loss: 2.366382598876953]\n",
      "[Epoch 7/40] [Batch 14/38] [D loss: 0.496529757976532] [G loss: 1.1809707880020142]\n",
      "[Epoch 7/40] [Batch 15/38] [D loss: 0.5131468772888184] [G loss: 1.666244626045227]\n",
      "[Epoch 7/40] [Batch 16/38] [D loss: 0.46580684185028076] [G loss: 1.3076703548431396]\n",
      "[Epoch 7/40] [Batch 17/38] [D loss: 0.4564874768257141] [G loss: 1.3803620338439941]\n",
      "[Epoch 7/40] [Batch 18/38] [D loss: 0.42258965969085693] [G loss: 1.4022128582000732]\n",
      "[Epoch 7/40] [Batch 19/38] [D loss: 0.4775960445404053] [G loss: 1.8577121496200562]\n",
      "[Epoch 7/40] [Batch 20/38] [D loss: 0.44779473543167114] [G loss: 1.0345603227615356]\n",
      "[Epoch 7/40] [Batch 21/38] [D loss: 0.39176028966903687] [G loss: 1.5081363916397095]\n",
      "[Epoch 7/40] [Batch 22/38] [D loss: 0.45111706852912903] [G loss: 1.5699043273925781]\n",
      "[Epoch 7/40] [Batch 23/38] [D loss: 0.4215349555015564] [G loss: 1.2459917068481445]\n",
      "[Epoch 7/40] [Batch 24/38] [D loss: 0.3846963047981262] [G loss: 1.6677316427230835]\n",
      "[Epoch 7/40] [Batch 25/38] [D loss: 0.5999774932861328] [G loss: 1.4484894275665283]\n",
      "[Epoch 7/40] [Batch 26/38] [D loss: 0.647447407245636] [G loss: 0.5950325727462769]\n",
      "[Epoch 7/40] [Batch 27/38] [D loss: 0.34905993938446045] [G loss: 1.8349305391311646]\n",
      "[Epoch 7/40] [Batch 28/38] [D loss: 0.41536980867385864] [G loss: 1.8200337886810303]\n",
      "[Epoch 7/40] [Batch 29/38] [D loss: 0.5581105947494507] [G loss: 0.9173566102981567]\n",
      "[Epoch 7/40] [Batch 30/38] [D loss: 0.420868456363678] [G loss: 1.6601887941360474]\n",
      "[Epoch 7/40] [Batch 31/38] [D loss: 0.38992398977279663] [G loss: 1.4630998373031616]\n",
      "[Epoch 7/40] [Batch 32/38] [D loss: 0.3413575291633606] [G loss: 1.1504069566726685]\n",
      "[Epoch 7/40] [Batch 33/38] [D loss: 0.40927332639694214] [G loss: 1.6776716709136963]\n",
      "[Epoch 7/40] [Batch 34/38] [D loss: 0.6338905096054077] [G loss: 1.260284423828125]\n",
      "[Epoch 7/40] [Batch 35/38] [D loss: 0.7105804085731506] [G loss: 1.0926778316497803]\n",
      "[Epoch 7/40] [Batch 36/38] [D loss: 0.6170480251312256] [G loss: 0.7210002541542053]\n",
      "[Epoch 7/40] [Batch 37/38] [D loss: 0.4623907208442688] [G loss: 1.6480411291122437]\n",
      "[Epoch 8/40] [Batch 0/38] [D loss: 0.43089020252227783] [G loss: 0.9958608746528625]\n",
      "[Epoch 8/40] [Batch 1/38] [D loss: 0.3946985602378845] [G loss: 1.715271234512329]\n",
      "[Epoch 8/40] [Batch 2/38] [D loss: 0.4680235683917999] [G loss: 1.6948797702789307]\n",
      "[Epoch 8/40] [Batch 3/38] [D loss: 0.4255906343460083] [G loss: 1.4239190816879272]\n",
      "[Epoch 8/40] [Batch 4/38] [D loss: 0.5250142216682434] [G loss: 1.331845998764038]\n",
      "[Epoch 8/40] [Batch 5/38] [D loss: 0.6461324691772461] [G loss: 1.637139081954956]\n",
      "[Epoch 8/40] [Batch 6/38] [D loss: 0.6873523592948914] [G loss: 0.699816107749939]\n",
      "[Epoch 8/40] [Batch 7/38] [D loss: 0.6529743075370789] [G loss: 0.9444913268089294]\n",
      "[Epoch 8/40] [Batch 8/38] [D loss: 0.8556663990020752] [G loss: 1.2344685792922974]\n",
      "[Epoch 8/40] [Batch 9/38] [D loss: 0.6361245512962341] [G loss: 0.6780277490615845]\n",
      "[Epoch 8/40] [Batch 10/38] [D loss: 0.5710087418556213] [G loss: 1.3682188987731934]\n",
      "[Epoch 8/40] [Batch 11/38] [D loss: 0.5095708966255188] [G loss: 1.3336801528930664]\n",
      "[Epoch 8/40] [Batch 12/38] [D loss: 0.3601577877998352] [G loss: 1.480949878692627]\n",
      "[Epoch 8/40] [Batch 13/38] [D loss: 0.44713789224624634] [G loss: 0.9537617564201355]\n",
      "[Epoch 8/40] [Batch 14/38] [D loss: 0.4869057536125183] [G loss: 1.259076714515686]\n",
      "[Epoch 8/40] [Batch 15/38] [D loss: 0.44752615690231323] [G loss: 1.682253122329712]\n",
      "[Epoch 8/40] [Batch 16/38] [D loss: 0.4453050196170807] [G loss: 1.3976821899414062]\n",
      "[Epoch 8/40] [Batch 17/38] [D loss: 0.374392569065094] [G loss: 1.9119117259979248]\n",
      "[Epoch 8/40] [Batch 18/38] [D loss: 0.5070298314094543] [G loss: 1.4598616361618042]\n",
      "[Epoch 8/40] [Batch 19/38] [D loss: 0.3824368417263031] [G loss: 1.3995894193649292]\n",
      "[Epoch 8/40] [Batch 20/38] [D loss: 0.6557919979095459] [G loss: 1.001134991645813]\n",
      "[Epoch 8/40] [Batch 21/38] [D loss: 0.6038864850997925] [G loss: 0.8343136310577393]\n",
      "[Epoch 8/40] [Batch 22/38] [D loss: 0.45252537727355957] [G loss: 1.8717896938323975]\n",
      "[Epoch 8/40] [Batch 23/38] [D loss: 0.4435276389122009] [G loss: 1.4543213844299316]\n",
      "[Epoch 8/40] [Batch 24/38] [D loss: 0.5014752149581909] [G loss: 1.3705037832260132]\n",
      "[Epoch 8/40] [Batch 25/38] [D loss: 0.5109928846359253] [G loss: 1.2026984691619873]\n",
      "[Epoch 8/40] [Batch 26/38] [D loss: 0.4372958540916443] [G loss: 1.2708560228347778]\n",
      "[Epoch 8/40] [Batch 27/38] [D loss: 0.4973532259464264] [G loss: 1.7591615915298462]\n",
      "[Epoch 8/40] [Batch 28/38] [D loss: 0.5361961722373962] [G loss: 1.243826150894165]\n",
      "[Epoch 8/40] [Batch 29/38] [D loss: 0.5763253569602966] [G loss: 0.9543576836585999]\n",
      "[Epoch 8/40] [Batch 30/38] [D loss: 0.5149412751197815] [G loss: 1.4329015016555786]\n",
      "[Epoch 8/40] [Batch 31/38] [D loss: 0.5175818204879761] [G loss: 1.6465840339660645]\n",
      "[Epoch 8/40] [Batch 32/38] [D loss: 0.5508534908294678] [G loss: 1.1818506717681885]\n",
      "[Epoch 8/40] [Batch 33/38] [D loss: 0.5953006148338318] [G loss: 0.8094972968101501]\n",
      "[Epoch 8/40] [Batch 34/38] [D loss: 0.536055326461792] [G loss: 1.347367763519287]\n",
      "[Epoch 8/40] [Batch 35/38] [D loss: 0.6946703791618347] [G loss: 1.1485297679901123]\n",
      "[Epoch 8/40] [Batch 36/38] [D loss: 0.6543177366256714] [G loss: 0.8655397891998291]\n",
      "[Epoch 8/40] [Batch 37/38] [D loss: 0.5106802582740784] [G loss: 1.0924071073532104]\n",
      "[Epoch 9/40] [Batch 0/38] [D loss: 0.6035050749778748] [G loss: 1.008538007736206]\n",
      "[Epoch 9/40] [Batch 1/38] [D loss: 0.43567755818367004] [G loss: 1.8214538097381592]\n",
      "[Epoch 9/40] [Batch 2/38] [D loss: 0.5041285753250122] [G loss: 1.3782358169555664]\n",
      "[Epoch 9/40] [Batch 3/38] [D loss: 0.5193406343460083] [G loss: 1.1163840293884277]\n",
      "[Epoch 9/40] [Batch 4/38] [D loss: 0.46390843391418457] [G loss: 1.3372721672058105]\n",
      "[Epoch 9/40] [Batch 5/38] [D loss: 0.5062823295593262] [G loss: 1.7706242799758911]\n",
      "[Epoch 9/40] [Batch 6/38] [D loss: 0.651740550994873] [G loss: 0.7465261220932007]\n",
      "[Epoch 9/40] [Batch 7/38] [D loss: 0.8242610692977905] [G loss: 0.6963056921958923]\n",
      "[Epoch 9/40] [Batch 8/38] [D loss: 0.6131892800331116] [G loss: 1.5621509552001953]\n",
      "[Epoch 9/40] [Batch 9/38] [D loss: 0.5945100784301758] [G loss: 1.0679152011871338]\n",
      "[Epoch 9/40] [Batch 10/38] [D loss: 0.5141794085502625] [G loss: 0.9998273849487305]\n",
      "[Epoch 9/40] [Batch 11/38] [D loss: 0.46046847105026245] [G loss: 1.8402800559997559]\n",
      "[Epoch 9/40] [Batch 12/38] [D loss: 0.5430314540863037] [G loss: 0.9337499141693115]\n",
      "[Epoch 9/40] [Batch 13/38] [D loss: 0.5974723100662231] [G loss: 0.7472705841064453]\n",
      "[Epoch 9/40] [Batch 14/38] [D loss: 0.5750293731689453] [G loss: 1.5931813716888428]\n",
      "[Epoch 9/40] [Batch 15/38] [D loss: 0.5183154344558716] [G loss: 1.1771173477172852]\n",
      "[Epoch 9/40] [Batch 16/38] [D loss: 0.5036745071411133] [G loss: 0.821832537651062]\n",
      "[Epoch 9/40] [Batch 17/38] [D loss: 0.48313388228416443] [G loss: 1.2707371711730957]\n",
      "[Epoch 9/40] [Batch 18/38] [D loss: 0.51902174949646] [G loss: 1.1914958953857422]\n",
      "[Epoch 9/40] [Batch 19/38] [D loss: 0.510512113571167] [G loss: 1.3903024196624756]\n",
      "[Epoch 9/40] [Batch 20/38] [D loss: 0.5186790227890015] [G loss: 0.9953276515007019]\n",
      "[Epoch 9/40] [Batch 21/38] [D loss: 0.6539459228515625] [G loss: 1.0950307846069336]\n",
      "[Epoch 9/40] [Batch 22/38] [D loss: 0.6090085506439209] [G loss: 0.9536611437797546]\n",
      "[Epoch 9/40] [Batch 23/38] [D loss: 0.5066202282905579] [G loss: 1.0663565397262573]\n",
      "[Epoch 9/40] [Batch 24/38] [D loss: 0.6241960525512695] [G loss: 1.034185767173767]\n",
      "[Epoch 9/40] [Batch 25/38] [D loss: 0.6197665333747864] [G loss: 1.3865840435028076]\n",
      "[Epoch 9/40] [Batch 26/38] [D loss: 0.6045759916305542] [G loss: 1.0941493511199951]\n",
      "[Epoch 9/40] [Batch 27/38] [D loss: 0.4071090817451477] [G loss: 1.0499356985092163]\n",
      "[Epoch 9/40] [Batch 28/38] [D loss: 0.5568045973777771] [G loss: 0.9261972904205322]\n",
      "[Epoch 9/40] [Batch 29/38] [D loss: 0.49354788661003113] [G loss: 1.0809530019760132]\n",
      "[Epoch 9/40] [Batch 30/38] [D loss: 0.5370826721191406] [G loss: 1.6700811386108398]\n",
      "[Epoch 9/40] [Batch 31/38] [D loss: 0.5600370764732361] [G loss: 0.9246506094932556]\n",
      "[Epoch 9/40] [Batch 32/38] [D loss: 0.6257147789001465] [G loss: 1.1294537782669067]\n",
      "[Epoch 9/40] [Batch 33/38] [D loss: 0.4574023485183716] [G loss: 1.2335087060928345]\n",
      "[Epoch 9/40] [Batch 34/38] [D loss: 0.5814610123634338] [G loss: 0.9929242134094238]\n",
      "[Epoch 9/40] [Batch 35/38] [D loss: 0.4757840633392334] [G loss: 1.5095984935760498]\n",
      "[Epoch 9/40] [Batch 36/38] [D loss: 0.5844056010246277] [G loss: 0.979670524597168]\n",
      "[Epoch 9/40] [Batch 37/38] [D loss: 0.6279664635658264] [G loss: 1.260352373123169]\n",
      "[Epoch 10/40] [Batch 0/38] [D loss: 0.4663052558898926] [G loss: 0.9600640535354614]\n",
      "[Epoch 10/40] [Batch 1/38] [D loss: 0.6570364236831665] [G loss: 1.379939079284668]\n",
      "[Epoch 10/40] [Batch 2/38] [D loss: 0.4772329330444336] [G loss: 0.8941107392311096]\n",
      "[Epoch 10/40] [Batch 3/38] [D loss: 0.634314775466919] [G loss: 0.9142606258392334]\n",
      "[Epoch 10/40] [Batch 4/38] [D loss: 0.6778929233551025] [G loss: 1.6197230815887451]\n",
      "[Epoch 10/40] [Batch 5/38] [D loss: 0.664707601070404] [G loss: 1.1450966596603394]\n",
      "[Epoch 10/40] [Batch 6/38] [D loss: 0.5990469455718994] [G loss: 0.662823498249054]\n",
      "[Epoch 10/40] [Batch 7/38] [D loss: 0.5998917818069458] [G loss: 1.1542973518371582]\n",
      "[Epoch 10/40] [Batch 8/38] [D loss: 0.5515674352645874] [G loss: 1.842005968093872]\n",
      "[Epoch 10/40] [Batch 9/38] [D loss: 0.5207775831222534] [G loss: 0.7944096326828003]\n",
      "[Epoch 10/40] [Batch 10/38] [D loss: 0.5226344466209412] [G loss: 0.8801444172859192]\n",
      "[Epoch 10/40] [Batch 11/38] [D loss: 0.4565480351448059] [G loss: 1.9476094245910645]\n",
      "[Epoch 10/40] [Batch 12/38] [D loss: 0.5173631906509399] [G loss: 1.1242634057998657]\n",
      "[Epoch 10/40] [Batch 13/38] [D loss: 0.5690712332725525] [G loss: 0.6896721124649048]\n",
      "[Epoch 10/40] [Batch 14/38] [D loss: 0.7036252021789551] [G loss: 1.307067632675171]\n",
      "[Epoch 10/40] [Batch 15/38] [D loss: 0.6640896797180176] [G loss: 1.0000386238098145]\n",
      "[Epoch 10/40] [Batch 16/38] [D loss: 0.5251818895339966] [G loss: 0.761115550994873]\n",
      "[Epoch 10/40] [Batch 17/38] [D loss: 0.4785746932029724] [G loss: 1.3200695514678955]\n",
      "[Epoch 10/40] [Batch 18/38] [D loss: 0.4718591570854187] [G loss: 1.455819010734558]\n",
      "[Epoch 10/40] [Batch 19/38] [D loss: 0.5632971525192261] [G loss: 1.0651566982269287]\n",
      "[Epoch 10/40] [Batch 20/38] [D loss: 0.5513325929641724] [G loss: 1.3831360340118408]\n",
      "[Epoch 10/40] [Batch 21/38] [D loss: 0.4927586615085602] [G loss: 1.05767023563385]\n",
      "[Epoch 10/40] [Batch 22/38] [D loss: 0.5725599527359009] [G loss: 1.2701127529144287]\n",
      "[Epoch 10/40] [Batch 23/38] [D loss: 0.5480746030807495] [G loss: 1.8006858825683594]\n",
      "[Epoch 10/40] [Batch 24/38] [D loss: 0.5921463370323181] [G loss: 0.8736793994903564]\n",
      "[Epoch 10/40] [Batch 25/38] [D loss: 0.6139904260635376] [G loss: 1.0025854110717773]\n",
      "[Epoch 10/40] [Batch 26/38] [D loss: 0.4661317467689514] [G loss: 1.146963119506836]\n",
      "[Epoch 10/40] [Batch 27/38] [D loss: 0.6154178977012634] [G loss: 1.3483721017837524]\n",
      "[Epoch 10/40] [Batch 28/38] [D loss: 0.43818598985671997] [G loss: 1.4369981288909912]\n",
      "[Epoch 10/40] [Batch 29/38] [D loss: 0.567887544631958] [G loss: 1.4475479125976562]\n",
      "[Epoch 10/40] [Batch 30/38] [D loss: 0.4894554018974304] [G loss: 1.0674357414245605]\n",
      "[Epoch 10/40] [Batch 31/38] [D loss: 0.48086968064308167] [G loss: 1.1293997764587402]\n",
      "[Epoch 10/40] [Batch 32/38] [D loss: 0.5392051339149475] [G loss: 1.1632885932922363]\n",
      "[Epoch 10/40] [Batch 33/38] [D loss: 0.7517882585525513] [G loss: 1.2505412101745605]\n",
      "[Epoch 10/40] [Batch 34/38] [D loss: 0.7692455053329468] [G loss: 0.6168993711471558]\n",
      "[Epoch 10/40] [Batch 35/38] [D loss: 0.5537356734275818] [G loss: 1.2404282093048096]\n",
      "[Epoch 10/40] [Batch 36/38] [D loss: 0.5646785497665405] [G loss: 1.1533479690551758]\n",
      "[Epoch 10/40] [Batch 37/38] [D loss: 0.5856636762619019] [G loss: 1.1566689014434814]\n",
      "[Epoch 11/40] [Batch 0/38] [D loss: 0.4676434397697449] [G loss: 1.220996618270874]\n",
      "[Epoch 11/40] [Batch 1/38] [D loss: 0.6737342476844788] [G loss: 0.8694839477539062]\n",
      "[Epoch 11/40] [Batch 2/38] [D loss: 0.6432194709777832] [G loss: 1.2384401559829712]\n",
      "[Epoch 11/40] [Batch 3/38] [D loss: 0.5048134326934814] [G loss: 1.2356157302856445]\n",
      "[Epoch 11/40] [Batch 4/38] [D loss: 0.5246446132659912] [G loss: 1.0726784467697144]\n",
      "[Epoch 11/40] [Batch 5/38] [D loss: 0.43749916553497314] [G loss: 1.7979121208190918]\n",
      "[Epoch 11/40] [Batch 6/38] [D loss: 0.4919762909412384] [G loss: 0.9755414128303528]\n",
      "[Epoch 11/40] [Batch 7/38] [D loss: 0.5696231126785278] [G loss: 0.9716609120368958]\n",
      "[Epoch 11/40] [Batch 8/38] [D loss: 0.5553741455078125] [G loss: 1.3126288652420044]\n",
      "[Epoch 11/40] [Batch 9/38] [D loss: 0.590872049331665] [G loss: 1.2392475605010986]\n",
      "[Epoch 11/40] [Batch 10/38] [D loss: 0.5820552110671997] [G loss: 1.1684905290603638]\n",
      "[Epoch 11/40] [Batch 11/38] [D loss: 0.6167297959327698] [G loss: 0.9731044173240662]\n",
      "[Epoch 11/40] [Batch 12/38] [D loss: 0.5667010545730591] [G loss: 1.5422039031982422]\n",
      "[Epoch 11/40] [Batch 13/38] [D loss: 0.45497211813926697] [G loss: 1.4070457220077515]\n",
      "[Epoch 11/40] [Batch 14/38] [D loss: 0.46731242537498474] [G loss: 1.037555456161499]\n",
      "[Epoch 11/40] [Batch 15/38] [D loss: 0.41859421133995056] [G loss: 1.644808053970337]\n",
      "[Epoch 11/40] [Batch 16/38] [D loss: 0.6255749464035034] [G loss: 0.9573131203651428]\n",
      "[Epoch 11/40] [Batch 17/38] [D loss: 0.49468356370925903] [G loss: 0.9588199853897095]\n",
      "[Epoch 11/40] [Batch 18/38] [D loss: 0.6746745109558105] [G loss: 0.8000224828720093]\n",
      "[Epoch 11/40] [Batch 19/38] [D loss: 0.4829743504524231] [G loss: 1.0513091087341309]\n",
      "[Epoch 11/40] [Batch 20/38] [D loss: 0.665565013885498] [G loss: 1.5434064865112305]\n",
      "[Epoch 11/40] [Batch 21/38] [D loss: 0.7562140822410583] [G loss: 0.5904824733734131]\n",
      "[Epoch 11/40] [Batch 22/38] [D loss: 0.7350136041641235] [G loss: 1.0737065076828003]\n",
      "[Epoch 11/40] [Batch 23/38] [D loss: 0.7501813769340515] [G loss: 0.986609935760498]\n",
      "[Epoch 11/40] [Batch 24/38] [D loss: 0.4714890122413635] [G loss: 1.5433601140975952]\n",
      "[Epoch 11/40] [Batch 25/38] [D loss: 0.40774309635162354] [G loss: 1.68752121925354]\n",
      "[Epoch 11/40] [Batch 26/38] [D loss: 0.44324496388435364] [G loss: 1.2588543891906738]\n",
      "[Epoch 11/40] [Batch 27/38] [D loss: 0.4009590148925781] [G loss: 1.3625688552856445]\n",
      "[Epoch 11/40] [Batch 28/38] [D loss: 0.42851653695106506] [G loss: 0.9675382971763611]\n",
      "[Epoch 11/40] [Batch 29/38] [D loss: 0.6350024938583374] [G loss: 1.2891933917999268]\n",
      "[Epoch 11/40] [Batch 30/38] [D loss: 0.49506813287734985] [G loss: 1.2634044885635376]\n",
      "[Epoch 11/40] [Batch 31/38] [D loss: 0.7540661096572876] [G loss: 0.6864510178565979]\n",
      "[Epoch 11/40] [Batch 32/38] [D loss: 0.635302722454071] [G loss: 1.4411711692810059]\n",
      "[Epoch 11/40] [Batch 33/38] [D loss: 0.49627843499183655] [G loss: 1.0900665521621704]\n",
      "[Epoch 11/40] [Batch 34/38] [D loss: 0.5462046265602112] [G loss: 1.6444669961929321]\n",
      "[Epoch 11/40] [Batch 35/38] [D loss: 0.5271720290184021] [G loss: 0.9122484922409058]\n",
      "[Epoch 11/40] [Batch 36/38] [D loss: 0.45026808977127075] [G loss: 1.4994004964828491]\n",
      "[Epoch 11/40] [Batch 37/38] [D loss: 0.31058821082115173] [G loss: 1.369522213935852]\n",
      "[Epoch 12/40] [Batch 0/38] [D loss: 0.5254970192909241] [G loss: 1.3153525590896606]\n",
      "[Epoch 12/40] [Batch 1/38] [D loss: 0.6877292394638062] [G loss: 0.9097659587860107]\n",
      "[Epoch 12/40] [Batch 2/38] [D loss: 0.789459228515625] [G loss: 0.767437219619751]\n",
      "[Epoch 12/40] [Batch 3/38] [D loss: 0.8530570268630981] [G loss: 1.348428726196289]\n",
      "[Epoch 12/40] [Batch 4/38] [D loss: 0.4122875928878784] [G loss: 1.3465794324874878]\n",
      "[Epoch 12/40] [Batch 5/38] [D loss: 0.4708403944969177] [G loss: 1.2738929986953735]\n",
      "[Epoch 12/40] [Batch 6/38] [D loss: 0.4275275468826294] [G loss: 1.5516818761825562]\n",
      "[Epoch 12/40] [Batch 7/38] [D loss: 0.45038071274757385] [G loss: 1.423558235168457]\n",
      "[Epoch 12/40] [Batch 8/38] [D loss: 0.5624259114265442] [G loss: 0.8685837984085083]\n",
      "[Epoch 12/40] [Batch 9/38] [D loss: 0.5061782598495483] [G loss: 1.3375109434127808]\n",
      "[Epoch 12/40] [Batch 10/38] [D loss: 0.4847172498703003] [G loss: 1.5717432498931885]\n",
      "[Epoch 12/40] [Batch 11/38] [D loss: 0.5568805932998657] [G loss: 1.4667211771011353]\n",
      "[Epoch 12/40] [Batch 12/38] [D loss: 0.5998693108558655] [G loss: 0.7192547917366028]\n",
      "[Epoch 12/40] [Batch 13/38] [D loss: 0.6487005949020386] [G loss: 1.2038809061050415]\n",
      "[Epoch 12/40] [Batch 14/38] [D loss: 0.8152598142623901] [G loss: 0.6834548115730286]\n",
      "[Epoch 12/40] [Batch 15/38] [D loss: 0.6297918558120728] [G loss: 1.4902842044830322]\n",
      "[Epoch 12/40] [Batch 16/38] [D loss: 0.532206654548645] [G loss: 1.2090758085250854]\n",
      "[Epoch 12/40] [Batch 17/38] [D loss: 0.6760505437850952] [G loss: 1.1290949583053589]\n",
      "[Epoch 12/40] [Batch 18/38] [D loss: 0.6951944828033447] [G loss: 1.1848832368850708]\n",
      "[Epoch 12/40] [Batch 19/38] [D loss: 0.5927687883377075] [G loss: 1.2166059017181396]\n",
      "[Epoch 12/40] [Batch 20/38] [D loss: 0.5750030875205994] [G loss: 1.4285364151000977]\n",
      "[Epoch 12/40] [Batch 21/38] [D loss: 0.4740424156188965] [G loss: 1.3003889322280884]\n",
      "[Epoch 12/40] [Batch 22/38] [D loss: 0.48768699169158936] [G loss: 1.198595643043518]\n",
      "[Epoch 12/40] [Batch 23/38] [D loss: 0.498751699924469] [G loss: 1.0565651655197144]\n",
      "[Epoch 12/40] [Batch 24/38] [D loss: 0.5028523802757263] [G loss: 1.3985624313354492]\n",
      "[Epoch 12/40] [Batch 25/38] [D loss: 0.3998242914676666] [G loss: 1.5818885564804077]\n",
      "[Epoch 12/40] [Batch 26/38] [D loss: 0.5368679761886597] [G loss: 1.1803053617477417]\n",
      "[Epoch 12/40] [Batch 27/38] [D loss: 0.5218708515167236] [G loss: 1.2739033699035645]\n",
      "[Epoch 12/40] [Batch 28/38] [D loss: 0.533664345741272] [G loss: 1.508718729019165]\n",
      "[Epoch 12/40] [Batch 29/38] [D loss: 0.5295785665512085] [G loss: 1.1601824760437012]\n",
      "[Epoch 12/40] [Batch 30/38] [D loss: 0.6371821761131287] [G loss: 0.9751813411712646]\n",
      "[Epoch 12/40] [Batch 31/38] [D loss: 0.5917266607284546] [G loss: 1.733034610748291]\n",
      "[Epoch 12/40] [Batch 32/38] [D loss: 0.5969341993331909] [G loss: 1.2878291606903076]\n",
      "[Epoch 12/40] [Batch 33/38] [D loss: 0.6509974002838135] [G loss: 0.9073722958564758]\n",
      "[Epoch 12/40] [Batch 34/38] [D loss: 0.5602013468742371] [G loss: 0.8536009788513184]\n",
      "[Epoch 12/40] [Batch 35/38] [D loss: 0.502875566482544] [G loss: 1.1876626014709473]\n",
      "[Epoch 12/40] [Batch 36/38] [D loss: 0.7219282984733582] [G loss: 1.1715031862258911]\n",
      "[Epoch 12/40] [Batch 37/38] [D loss: 0.5706241726875305] [G loss: 1.4220651388168335]\n",
      "[Epoch 13/40] [Batch 0/38] [D loss: 0.6700291633605957] [G loss: 0.9648460149765015]\n",
      "[Epoch 13/40] [Batch 1/38] [D loss: 0.6149277687072754] [G loss: 0.8273460865020752]\n",
      "[Epoch 13/40] [Batch 2/38] [D loss: 0.4093472361564636] [G loss: 1.3338139057159424]\n",
      "[Epoch 13/40] [Batch 3/38] [D loss: 0.48741352558135986] [G loss: 1.7548203468322754]\n",
      "[Epoch 13/40] [Batch 4/38] [D loss: 0.5862682461738586] [G loss: 0.9361065626144409]\n",
      "[Epoch 13/40] [Batch 5/38] [D loss: 0.5475311279296875] [G loss: 1.3717601299285889]\n",
      "[Epoch 13/40] [Batch 6/38] [D loss: 0.549712598323822] [G loss: 1.1815935373306274]\n",
      "[Epoch 13/40] [Batch 7/38] [D loss: 0.5598827600479126] [G loss: 1.1960458755493164]\n",
      "[Epoch 13/40] [Batch 8/38] [D loss: 0.49177536368370056] [G loss: 1.230838418006897]\n",
      "[Epoch 13/40] [Batch 9/38] [D loss: 0.49196550250053406] [G loss: 1.3773466348648071]\n",
      "[Epoch 13/40] [Batch 10/38] [D loss: 0.44597288966178894] [G loss: 1.264161467552185]\n",
      "[Epoch 13/40] [Batch 11/38] [D loss: 0.5465404391288757] [G loss: 1.0342862606048584]\n",
      "[Epoch 13/40] [Batch 12/38] [D loss: 0.5318000912666321] [G loss: 1.0217407941818237]\n",
      "[Epoch 13/40] [Batch 13/38] [D loss: 0.45216506719589233] [G loss: 1.0804307460784912]\n",
      "[Epoch 13/40] [Batch 14/38] [D loss: 0.6468520760536194] [G loss: 1.4780076742172241]\n",
      "[Epoch 13/40] [Batch 15/38] [D loss: 0.5493382215499878] [G loss: 1.0980429649353027]\n",
      "[Epoch 13/40] [Batch 16/38] [D loss: 0.6075895428657532] [G loss: 0.9461548924446106]\n",
      "[Epoch 13/40] [Batch 17/38] [D loss: 0.5408135652542114] [G loss: 1.1959248781204224]\n",
      "[Epoch 13/40] [Batch 18/38] [D loss: 0.5706286430358887] [G loss: 1.16551673412323]\n",
      "[Epoch 13/40] [Batch 19/38] [D loss: 0.48014265298843384] [G loss: 1.5519214868545532]\n",
      "[Epoch 13/40] [Batch 20/38] [D loss: 0.4436339735984802] [G loss: 1.157245397567749]\n",
      "[Epoch 13/40] [Batch 21/38] [D loss: 0.3519120514392853] [G loss: 1.5660957098007202]\n",
      "[Epoch 13/40] [Batch 22/38] [D loss: 0.45166024565696716] [G loss: 2.1898837089538574]\n",
      "[Epoch 13/40] [Batch 23/38] [D loss: 0.39568382501602173] [G loss: 1.128444790840149]\n",
      "[Epoch 13/40] [Batch 24/38] [D loss: 0.49530595541000366] [G loss: 1.3182032108306885]\n",
      "[Epoch 13/40] [Batch 25/38] [D loss: 0.4783034026622772] [G loss: 1.6251091957092285]\n",
      "[Epoch 13/40] [Batch 26/38] [D loss: 0.6850700378417969] [G loss: 1.3211675882339478]\n",
      "[Epoch 13/40] [Batch 27/38] [D loss: 0.5302774906158447] [G loss: 1.4238135814666748]\n",
      "[Epoch 13/40] [Batch 28/38] [D loss: 0.3251297175884247] [G loss: 1.3437118530273438]\n",
      "[Epoch 13/40] [Batch 29/38] [D loss: 0.5471310615539551] [G loss: 1.4028046131134033]\n",
      "[Epoch 13/40] [Batch 30/38] [D loss: 0.5557571649551392] [G loss: 1.2061164379119873]\n",
      "[Epoch 13/40] [Batch 31/38] [D loss: 0.6620269417762756] [G loss: 0.7510384321212769]\n",
      "[Epoch 13/40] [Batch 32/38] [D loss: 0.6709998846054077] [G loss: 0.8458695411682129]\n",
      "[Epoch 13/40] [Batch 33/38] [D loss: 0.7344640493392944] [G loss: 1.728896975517273]\n",
      "[Epoch 13/40] [Batch 34/38] [D loss: 0.48823583126068115] [G loss: 1.0383466482162476]\n",
      "[Epoch 13/40] [Batch 35/38] [D loss: 0.5266416072845459] [G loss: 0.8435366153717041]\n",
      "[Epoch 13/40] [Batch 36/38] [D loss: 0.49481073021888733] [G loss: 1.5372097492218018]\n",
      "[Epoch 13/40] [Batch 37/38] [D loss: 0.4483853578567505] [G loss: 1.5702600479125977]\n",
      "[Epoch 14/40] [Batch 0/38] [D loss: 0.46950262784957886] [G loss: 0.9247841835021973]\n",
      "[Epoch 14/40] [Batch 1/38] [D loss: 0.4620320200920105] [G loss: 1.3467475175857544]\n",
      "[Epoch 14/40] [Batch 2/38] [D loss: 0.5059734582901001] [G loss: 1.4974865913391113]\n",
      "[Epoch 14/40] [Batch 3/38] [D loss: 0.6225595474243164] [G loss: 0.7405436038970947]\n",
      "[Epoch 14/40] [Batch 4/38] [D loss: 0.6213536262512207] [G loss: 1.1773865222930908]\n",
      "[Epoch 14/40] [Batch 5/38] [D loss: 0.5242437124252319] [G loss: 1.1802983283996582]\n",
      "[Epoch 14/40] [Batch 6/38] [D loss: 0.5342950224876404] [G loss: 1.2491315603256226]\n",
      "[Epoch 14/40] [Batch 7/38] [D loss: 0.5478724241256714] [G loss: 1.1594754457473755]\n",
      "[Epoch 14/40] [Batch 8/38] [D loss: 0.5077377557754517] [G loss: 1.0920721292495728]\n",
      "[Epoch 14/40] [Batch 9/38] [D loss: 0.5533137321472168] [G loss: 1.5003489255905151]\n",
      "[Epoch 14/40] [Batch 10/38] [D loss: 0.49383029341697693] [G loss: 1.1220529079437256]\n",
      "[Epoch 14/40] [Batch 11/38] [D loss: 0.46111953258514404] [G loss: 1.6206817626953125]\n",
      "[Epoch 14/40] [Batch 12/38] [D loss: 0.6422738432884216] [G loss: 1.4549667835235596]\n",
      "[Epoch 14/40] [Batch 13/38] [D loss: 0.4994546175003052] [G loss: 1.3357336521148682]\n",
      "[Epoch 14/40] [Batch 14/38] [D loss: 0.5989443063735962] [G loss: 0.6478937268257141]\n",
      "[Epoch 14/40] [Batch 15/38] [D loss: 0.6099881529808044] [G loss: 1.1376750469207764]\n",
      "[Epoch 14/40] [Batch 16/38] [D loss: 0.5242083072662354] [G loss: 1.6214218139648438]\n",
      "[Epoch 14/40] [Batch 17/38] [D loss: 0.42563438415527344] [G loss: 1.1692421436309814]\n",
      "[Epoch 14/40] [Batch 18/38] [D loss: 0.4140281677246094] [G loss: 1.3612182140350342]\n",
      "[Epoch 14/40] [Batch 19/38] [D loss: 0.479072242975235] [G loss: 1.322596788406372]\n",
      "[Epoch 14/40] [Batch 20/38] [D loss: 0.526520848274231] [G loss: 1.069556474685669]\n",
      "[Epoch 14/40] [Batch 21/38] [D loss: 0.3685978651046753] [G loss: 1.393521785736084]\n",
      "[Epoch 14/40] [Batch 22/38] [D loss: 0.5923665761947632] [G loss: 1.290269374847412]\n",
      "[Epoch 14/40] [Batch 23/38] [D loss: 0.5336138010025024] [G loss: 0.9984805583953857]\n",
      "[Epoch 14/40] [Batch 24/38] [D loss: 0.491531640291214] [G loss: 1.1328182220458984]\n",
      "[Epoch 14/40] [Batch 25/38] [D loss: 0.5026794075965881] [G loss: 1.2445380687713623]\n",
      "[Epoch 14/40] [Batch 26/38] [D loss: 0.5018559098243713] [G loss: 1.8132678270339966]\n",
      "[Epoch 14/40] [Batch 27/38] [D loss: 0.5136113166809082] [G loss: 1.0093865394592285]\n",
      "[Epoch 14/40] [Batch 28/38] [D loss: 0.4452919363975525] [G loss: 1.1628122329711914]\n",
      "[Epoch 14/40] [Batch 29/38] [D loss: 0.48561275005340576] [G loss: 1.1427992582321167]\n",
      "[Epoch 14/40] [Batch 30/38] [D loss: 0.4912329316139221] [G loss: 1.2259058952331543]\n",
      "[Epoch 14/40] [Batch 31/38] [D loss: 0.4965125024318695] [G loss: 1.1594774723052979]\n",
      "[Epoch 14/40] [Batch 32/38] [D loss: 0.5495980978012085] [G loss: 1.1285104751586914]\n",
      "[Epoch 14/40] [Batch 33/38] [D loss: 0.4792546033859253] [G loss: 1.0539623498916626]\n",
      "[Epoch 14/40] [Batch 34/38] [D loss: 0.4384080171585083] [G loss: 1.8876502513885498]\n",
      "[Epoch 14/40] [Batch 35/38] [D loss: 0.6748789548873901] [G loss: 1.6729835271835327]\n",
      "[Epoch 14/40] [Batch 36/38] [D loss: 0.5946848392486572] [G loss: 0.6518082022666931]\n",
      "[Epoch 14/40] [Batch 37/38] [D loss: 0.3392060399055481] [G loss: 1.9477322101593018]\n",
      "[Epoch 15/40] [Batch 0/38] [D loss: 0.4525727927684784] [G loss: 1.396681308746338]\n",
      "[Epoch 15/40] [Batch 1/38] [D loss: 0.5106499195098877] [G loss: 1.0017622709274292]\n",
      "[Epoch 15/40] [Batch 2/38] [D loss: 0.7092565894126892] [G loss: 0.9580087661743164]\n",
      "[Epoch 15/40] [Batch 3/38] [D loss: 0.727125883102417] [G loss: 2.0060627460479736]\n",
      "[Epoch 15/40] [Batch 4/38] [D loss: 0.764746904373169] [G loss: 0.9276745915412903]\n",
      "[Epoch 15/40] [Batch 5/38] [D loss: 0.4745023250579834] [G loss: 1.2368346452713013]\n",
      "[Epoch 15/40] [Batch 6/38] [D loss: 0.7156603336334229] [G loss: 1.2977192401885986]\n",
      "[Epoch 15/40] [Batch 7/38] [D loss: 0.7461142539978027] [G loss: 1.0065102577209473]\n",
      "[Epoch 15/40] [Batch 8/38] [D loss: 0.5862581729888916] [G loss: 0.9462559223175049]\n",
      "[Epoch 15/40] [Batch 9/38] [D loss: 0.4962219297885895] [G loss: 1.7141727209091187]\n",
      "[Epoch 15/40] [Batch 10/38] [D loss: 0.42623916268348694] [G loss: 1.229581594467163]\n",
      "[Epoch 15/40] [Batch 11/38] [D loss: 0.7085193395614624] [G loss: 0.64249587059021]\n",
      "[Epoch 15/40] [Batch 12/38] [D loss: 0.49930480122566223] [G loss: 1.2695107460021973]\n",
      "[Epoch 15/40] [Batch 13/38] [D loss: 0.5053566694259644] [G loss: 1.2839882373809814]\n",
      "[Epoch 15/40] [Batch 14/38] [D loss: 0.4853346347808838] [G loss: 1.6267229318618774]\n",
      "[Epoch 15/40] [Batch 15/38] [D loss: 0.46954119205474854] [G loss: 1.0952188968658447]\n",
      "[Epoch 15/40] [Batch 16/38] [D loss: 0.5578898191452026] [G loss: 1.1490906476974487]\n",
      "[Epoch 15/40] [Batch 17/38] [D loss: 0.5362943410873413] [G loss: 1.5333281755447388]\n",
      "[Epoch 15/40] [Batch 18/38] [D loss: 0.5167273283004761] [G loss: 1.0228134393692017]\n",
      "[Epoch 15/40] [Batch 19/38] [D loss: 0.560642421245575] [G loss: 0.8887308835983276]\n",
      "[Epoch 15/40] [Batch 20/38] [D loss: 0.7066668272018433] [G loss: 1.4774174690246582]\n",
      "[Epoch 15/40] [Batch 21/38] [D loss: 0.47921791672706604] [G loss: 1.1238118410110474]\n",
      "[Epoch 15/40] [Batch 22/38] [D loss: 0.5380635261535645] [G loss: 1.3007787466049194]\n",
      "[Epoch 15/40] [Batch 23/38] [D loss: 0.45911291241645813] [G loss: 1.2315970659255981]\n",
      "[Epoch 15/40] [Batch 24/38] [D loss: 0.48926910758018494] [G loss: 1.607492446899414]\n",
      "[Epoch 15/40] [Batch 25/38] [D loss: 0.5269672870635986] [G loss: 1.3688908815383911]\n",
      "[Epoch 15/40] [Batch 26/38] [D loss: 0.5343202948570251] [G loss: 0.7728497982025146]\n",
      "[Epoch 15/40] [Batch 27/38] [D loss: 0.45697855949401855] [G loss: 1.3035366535186768]\n",
      "[Epoch 15/40] [Batch 28/38] [D loss: 0.5645614266395569] [G loss: 1.473006248474121]\n",
      "[Epoch 15/40] [Batch 29/38] [D loss: 0.5514265298843384] [G loss: 1.0713223218917847]\n",
      "[Epoch 15/40] [Batch 30/38] [D loss: 0.433349609375] [G loss: 1.121486783027649]\n",
      "[Epoch 15/40] [Batch 31/38] [D loss: 0.4671041965484619] [G loss: 1.379212498664856]\n",
      "[Epoch 15/40] [Batch 32/38] [D loss: 0.7210415601730347] [G loss: 0.8688404560089111]\n",
      "[Epoch 15/40] [Batch 33/38] [D loss: 0.5195546746253967] [G loss: 1.1634382009506226]\n",
      "[Epoch 15/40] [Batch 34/38] [D loss: 0.7433854937553406] [G loss: 1.4128652811050415]\n",
      "[Epoch 15/40] [Batch 35/38] [D loss: 0.7226496934890747] [G loss: 0.9468001127243042]\n",
      "[Epoch 15/40] [Batch 36/38] [D loss: 0.5232866406440735] [G loss: 0.8131583333015442]\n",
      "[Epoch 15/40] [Batch 37/38] [D loss: 0.5334901213645935] [G loss: 2.533067464828491]\n",
      "[Epoch 16/40] [Batch 0/38] [D loss: 0.4463880956172943] [G loss: 0.8461512923240662]\n",
      "[Epoch 16/40] [Batch 1/38] [D loss: 0.569242537021637] [G loss: 0.9774098992347717]\n",
      "[Epoch 16/40] [Batch 2/38] [D loss: 0.4411391019821167] [G loss: 2.4337546825408936]\n",
      "[Epoch 16/40] [Batch 3/38] [D loss: 0.5644298791885376] [G loss: 1.2715381383895874]\n",
      "[Epoch 16/40] [Batch 4/38] [D loss: 0.41536349058151245] [G loss: 0.9876788258552551]\n",
      "[Epoch 16/40] [Batch 5/38] [D loss: 0.4608771800994873] [G loss: 1.8945202827453613]\n",
      "[Epoch 16/40] [Batch 6/38] [D loss: 0.4612436294555664] [G loss: 1.3588461875915527]\n",
      "[Epoch 16/40] [Batch 7/38] [D loss: 0.5639500617980957] [G loss: 0.7733663320541382]\n",
      "[Epoch 16/40] [Batch 8/38] [D loss: 0.5413399338722229] [G loss: 1.7662521600723267]\n",
      "[Epoch 16/40] [Batch 9/38] [D loss: 0.5734730958938599] [G loss: 1.1308472156524658]\n",
      "[Epoch 16/40] [Batch 10/38] [D loss: 0.9241868257522583] [G loss: 0.6361400485038757]\n",
      "[Epoch 16/40] [Batch 11/38] [D loss: 0.7087990641593933] [G loss: 1.121914029121399]\n",
      "[Epoch 16/40] [Batch 12/38] [D loss: 0.5829317569732666] [G loss: 1.949694037437439]\n",
      "[Epoch 16/40] [Batch 13/38] [D loss: 0.6307384371757507] [G loss: 0.9966834187507629]\n",
      "[Epoch 16/40] [Batch 14/38] [D loss: 0.3726985454559326] [G loss: 1.0296411514282227]\n",
      "[Epoch 16/40] [Batch 15/38] [D loss: 0.4287871718406677] [G loss: 1.626517653465271]\n",
      "[Epoch 16/40] [Batch 16/38] [D loss: 0.6464423537254333] [G loss: 1.1202281713485718]\n",
      "[Epoch 16/40] [Batch 17/38] [D loss: 0.4556505084037781] [G loss: 1.2983602285385132]\n",
      "[Epoch 16/40] [Batch 18/38] [D loss: 0.4486408829689026] [G loss: 1.3307698965072632]\n",
      "[Epoch 16/40] [Batch 19/38] [D loss: 0.5305029153823853] [G loss: 1.0872410535812378]\n",
      "[Epoch 16/40] [Batch 20/38] [D loss: 0.5224527716636658] [G loss: 1.4737327098846436]\n",
      "[Epoch 16/40] [Batch 21/38] [D loss: 0.5164609551429749] [G loss: 1.1929508447647095]\n",
      "[Epoch 16/40] [Batch 22/38] [D loss: 0.5347795486450195] [G loss: 0.9632805585861206]\n",
      "[Epoch 16/40] [Batch 23/38] [D loss: 0.3897266983985901] [G loss: 1.5318931341171265]\n",
      "[Epoch 16/40] [Batch 24/38] [D loss: 0.41727668046951294] [G loss: 1.4346047639846802]\n",
      "[Epoch 16/40] [Batch 25/38] [D loss: 0.521346926689148] [G loss: 1.1080920696258545]\n",
      "[Epoch 16/40] [Batch 26/38] [D loss: 0.5541476011276245] [G loss: 1.7852306365966797]\n",
      "[Epoch 16/40] [Batch 27/38] [D loss: 0.669630765914917] [G loss: 0.9870908260345459]\n",
      "[Epoch 16/40] [Batch 28/38] [D loss: 0.4117659330368042] [G loss: 1.096640706062317]\n",
      "[Epoch 16/40] [Batch 29/38] [D loss: 0.3538828194141388] [G loss: 1.8699653148651123]\n",
      "[Epoch 16/40] [Batch 30/38] [D loss: 0.4183891713619232] [G loss: 1.0814710855484009]\n",
      "[Epoch 16/40] [Batch 31/38] [D loss: 0.3951755166053772] [G loss: 1.4415171146392822]\n",
      "[Epoch 16/40] [Batch 32/38] [D loss: 0.43453311920166016] [G loss: 1.1855344772338867]\n",
      "[Epoch 16/40] [Batch 33/38] [D loss: 0.4325500726699829] [G loss: 1.7787237167358398]\n",
      "[Epoch 16/40] [Batch 34/38] [D loss: 0.5278668403625488] [G loss: 1.2406936883926392]\n",
      "[Epoch 16/40] [Batch 35/38] [D loss: 0.40290355682373047] [G loss: 1.0369569063186646]\n",
      "[Epoch 16/40] [Batch 36/38] [D loss: 0.4302142262458801] [G loss: 1.639273762702942]\n",
      "[Epoch 16/40] [Batch 37/38] [D loss: 0.49372681975364685] [G loss: 2.005627155303955]\n",
      "[Epoch 17/40] [Batch 0/38] [D loss: 0.6847207546234131] [G loss: 0.6510874032974243]\n",
      "[Epoch 17/40] [Batch 1/38] [D loss: 0.5566184520721436] [G loss: 1.1926543712615967]\n",
      "[Epoch 17/40] [Batch 2/38] [D loss: 0.486561119556427] [G loss: 1.686060905456543]\n",
      "[Epoch 17/40] [Batch 3/38] [D loss: 0.5430916547775269] [G loss: 1.3327343463897705]\n",
      "[Epoch 17/40] [Batch 4/38] [D loss: 0.4667509198188782] [G loss: 1.4236711263656616]\n",
      "[Epoch 17/40] [Batch 5/38] [D loss: 0.5098186731338501] [G loss: 1.8758728504180908]\n",
      "[Epoch 17/40] [Batch 6/38] [D loss: 0.4062415361404419] [G loss: 1.3149882555007935]\n",
      "[Epoch 17/40] [Batch 7/38] [D loss: 0.506877064704895] [G loss: 1.309794306755066]\n",
      "[Epoch 17/40] [Batch 8/38] [D loss: 0.35129204392433167] [G loss: 1.356249213218689]\n",
      "[Epoch 17/40] [Batch 9/38] [D loss: 0.3867403566837311] [G loss: 1.6045281887054443]\n",
      "[Epoch 17/40] [Batch 10/38] [D loss: 0.42180392146110535] [G loss: 1.580806016921997]\n",
      "[Epoch 17/40] [Batch 11/38] [D loss: 0.3626939058303833] [G loss: 1.3896327018737793]\n",
      "[Epoch 17/40] [Batch 12/38] [D loss: 0.5582199692726135] [G loss: 1.8745390176773071]\n",
      "[Epoch 17/40] [Batch 13/38] [D loss: 0.7039431929588318] [G loss: 0.7245498895645142]\n",
      "[Epoch 17/40] [Batch 14/38] [D loss: 0.6035639643669128] [G loss: 1.0069756507873535]\n",
      "[Epoch 17/40] [Batch 15/38] [D loss: 0.6430127620697021] [G loss: 1.148791790008545]\n",
      "[Epoch 17/40] [Batch 16/38] [D loss: 0.422862708568573] [G loss: 1.2610065937042236]\n",
      "[Epoch 17/40] [Batch 17/38] [D loss: 0.5552747845649719] [G loss: 1.1463900804519653]\n",
      "[Epoch 17/40] [Batch 18/38] [D loss: 0.4353996813297272] [G loss: 1.7539547681808472]\n",
      "[Epoch 17/40] [Batch 19/38] [D loss: 0.4950316250324249] [G loss: 1.1939691305160522]\n",
      "[Epoch 17/40] [Batch 20/38] [D loss: 0.27794191241264343] [G loss: 1.5971827507019043]\n",
      "[Epoch 17/40] [Batch 21/38] [D loss: 0.551093339920044] [G loss: 1.308770775794983]\n",
      "[Epoch 17/40] [Batch 22/38] [D loss: 0.49564045667648315] [G loss: 1.1919071674346924]\n",
      "[Epoch 17/40] [Batch 23/38] [D loss: 0.4021225571632385] [G loss: 1.2990851402282715]\n",
      "[Epoch 17/40] [Batch 24/38] [D loss: 0.49928170442581177] [G loss: 1.9374048709869385]\n",
      "[Epoch 17/40] [Batch 25/38] [D loss: 0.6995269656181335] [G loss: 0.5775353312492371]\n",
      "[Epoch 17/40] [Batch 26/38] [D loss: 0.5825439691543579] [G loss: 1.2685117721557617]\n",
      "[Epoch 17/40] [Batch 27/38] [D loss: 0.6821495890617371] [G loss: 1.2636189460754395]\n",
      "[Epoch 17/40] [Batch 28/38] [D loss: 0.6357231140136719] [G loss: 0.8967792391777039]\n",
      "[Epoch 17/40] [Batch 29/38] [D loss: 0.5261150598526001] [G loss: 1.6124553680419922]\n",
      "[Epoch 17/40] [Batch 30/38] [D loss: 0.5156333446502686] [G loss: 1.312699556350708]\n",
      "[Epoch 17/40] [Batch 31/38] [D loss: 0.6239343881607056] [G loss: 0.9532408714294434]\n",
      "[Epoch 17/40] [Batch 32/38] [D loss: 0.606602668762207] [G loss: 1.2192795276641846]\n",
      "[Epoch 17/40] [Batch 33/38] [D loss: 0.5350871682167053] [G loss: 1.2855173349380493]\n",
      "[Epoch 17/40] [Batch 34/38] [D loss: 0.6460780501365662] [G loss: 1.1678335666656494]\n",
      "[Epoch 17/40] [Batch 35/38] [D loss: 0.5931910276412964] [G loss: 0.8197426199913025]\n",
      "[Epoch 17/40] [Batch 36/38] [D loss: 0.5923701524734497] [G loss: 1.3165907859802246]\n",
      "[Epoch 17/40] [Batch 37/38] [D loss: 0.7515591382980347] [G loss: 1.0436278581619263]\n",
      "[Epoch 18/40] [Batch 0/38] [D loss: 0.4096301794052124] [G loss: 1.4284968376159668]\n",
      "[Epoch 18/40] [Batch 1/38] [D loss: 0.462727427482605] [G loss: 1.03685462474823]\n",
      "[Epoch 18/40] [Batch 2/38] [D loss: 0.5113922953605652] [G loss: 1.4420782327651978]\n",
      "[Epoch 18/40] [Batch 3/38] [D loss: 0.592136561870575] [G loss: 1.3606035709381104]\n",
      "[Epoch 18/40] [Batch 4/38] [D loss: 0.6997512578964233] [G loss: 0.5076197385787964]\n",
      "[Epoch 18/40] [Batch 5/38] [D loss: 0.5232023000717163] [G loss: 1.3386712074279785]\n",
      "[Epoch 18/40] [Batch 6/38] [D loss: 0.531386137008667] [G loss: 1.9593676328659058]\n",
      "[Epoch 18/40] [Batch 7/38] [D loss: 0.4992358684539795] [G loss: 1.1357989311218262]\n",
      "[Epoch 18/40] [Batch 8/38] [D loss: 0.4697439670562744] [G loss: 0.8884345293045044]\n",
      "[Epoch 18/40] [Batch 9/38] [D loss: 0.6588321924209595] [G loss: 1.230886697769165]\n",
      "[Epoch 18/40] [Batch 10/38] [D loss: 0.6041329503059387] [G loss: 1.5139137506484985]\n",
      "[Epoch 18/40] [Batch 11/38] [D loss: 0.6194202899932861] [G loss: 1.351279377937317]\n",
      "[Epoch 18/40] [Batch 12/38] [D loss: 0.6286336183547974] [G loss: 0.7952181696891785]\n",
      "[Epoch 18/40] [Batch 13/38] [D loss: 0.7558407783508301] [G loss: 0.9172039031982422]\n",
      "[Epoch 18/40] [Batch 14/38] [D loss: 0.40555766224861145] [G loss: 1.495863914489746]\n",
      "[Epoch 18/40] [Batch 15/38] [D loss: 0.5597114562988281] [G loss: 1.628423810005188]\n",
      "[Epoch 18/40] [Batch 16/38] [D loss: 0.496210515499115] [G loss: 0.8093461990356445]\n",
      "[Epoch 18/40] [Batch 17/38] [D loss: 0.4014984965324402] [G loss: 1.0308080911636353]\n",
      "[Epoch 18/40] [Batch 18/38] [D loss: 0.48184579610824585] [G loss: 1.8013867139816284]\n",
      "[Epoch 18/40] [Batch 19/38] [D loss: 0.49701765179634094] [G loss: 1.4151252508163452]\n",
      "[Epoch 18/40] [Batch 20/38] [D loss: 0.5714476108551025] [G loss: 0.8504818677902222]\n",
      "[Epoch 18/40] [Batch 21/38] [D loss: 0.6089444756507874] [G loss: 1.0877857208251953]\n",
      "[Epoch 18/40] [Batch 22/38] [D loss: 0.6785026788711548] [G loss: 1.3864798545837402]\n",
      "[Epoch 18/40] [Batch 23/38] [D loss: 0.667569100856781] [G loss: 0.7879452705383301]\n",
      "[Epoch 18/40] [Batch 24/38] [D loss: 0.6097086668014526] [G loss: 1.1895545721054077]\n",
      "[Epoch 18/40] [Batch 25/38] [D loss: 0.505540132522583] [G loss: 1.678733468055725]\n",
      "[Epoch 18/40] [Batch 26/38] [D loss: 0.5369687080383301] [G loss: 1.0891163349151611]\n",
      "[Epoch 18/40] [Batch 27/38] [D loss: 0.5384151935577393] [G loss: 0.9048265814781189]\n",
      "[Epoch 18/40] [Batch 28/38] [D loss: 0.40319252014160156] [G loss: 2.3623406887054443]\n",
      "[Epoch 18/40] [Batch 29/38] [D loss: 0.5295825004577637] [G loss: 1.1902796030044556]\n",
      "[Epoch 18/40] [Batch 30/38] [D loss: 0.39290404319763184] [G loss: 1.0398304462432861]\n",
      "[Epoch 18/40] [Batch 31/38] [D loss: 0.5663536787033081] [G loss: 1.254101276397705]\n",
      "[Epoch 18/40] [Batch 32/38] [D loss: 0.6281090974807739] [G loss: 1.4648122787475586]\n",
      "[Epoch 18/40] [Batch 33/38] [D loss: 0.6532073020935059] [G loss: 0.7741346955299377]\n",
      "[Epoch 18/40] [Batch 34/38] [D loss: 0.636908769607544] [G loss: 1.5930720567703247]\n",
      "[Epoch 18/40] [Batch 35/38] [D loss: 0.6788190603256226] [G loss: 1.1740089654922485]\n",
      "[Epoch 18/40] [Batch 36/38] [D loss: 0.7359588146209717] [G loss: 0.5679157376289368]\n",
      "[Epoch 18/40] [Batch 37/38] [D loss: 0.48994433879852295] [G loss: 1.7220475673675537]\n",
      "[Epoch 19/40] [Batch 0/38] [D loss: 0.6247197389602661] [G loss: 1.495782732963562]\n",
      "[Epoch 19/40] [Batch 1/38] [D loss: 0.4332895278930664] [G loss: 1.1324440240859985]\n",
      "[Epoch 19/40] [Batch 2/38] [D loss: 0.48742905259132385] [G loss: 1.3009463548660278]\n",
      "[Epoch 19/40] [Batch 3/38] [D loss: 0.4750850200653076] [G loss: 1.4450232982635498]\n",
      "[Epoch 19/40] [Batch 4/38] [D loss: 0.41944700479507446] [G loss: 1.4237619638442993]\n",
      "[Epoch 19/40] [Batch 5/38] [D loss: 0.4411311745643616] [G loss: 1.1444886922836304]\n",
      "[Epoch 19/40] [Batch 6/38] [D loss: 0.5098809003829956] [G loss: 1.3999134302139282]\n",
      "[Epoch 19/40] [Batch 7/38] [D loss: 0.5664157271385193] [G loss: 1.2662707567214966]\n",
      "[Epoch 19/40] [Batch 8/38] [D loss: 0.5523939728736877] [G loss: 0.9254063367843628]\n",
      "[Epoch 19/40] [Batch 9/38] [D loss: 0.5486243963241577] [G loss: 1.4905275106430054]\n",
      "[Epoch 19/40] [Batch 10/38] [D loss: 0.6808968186378479] [G loss: 1.5306322574615479]\n",
      "[Epoch 19/40] [Batch 11/38] [D loss: 0.6617146730422974] [G loss: 0.9702531695365906]\n",
      "[Epoch 19/40] [Batch 12/38] [D loss: 0.5481024980545044] [G loss: 1.1109960079193115]\n",
      "[Epoch 19/40] [Batch 13/38] [D loss: 0.5196609497070312] [G loss: 1.8455454111099243]\n",
      "[Epoch 19/40] [Batch 14/38] [D loss: 0.5837791562080383] [G loss: 1.2047736644744873]\n",
      "[Epoch 19/40] [Batch 15/38] [D loss: 0.5291917324066162] [G loss: 0.8250579237937927]\n",
      "[Epoch 19/40] [Batch 16/38] [D loss: 0.42020437121391296] [G loss: 2.0115206241607666]\n",
      "[Epoch 19/40] [Batch 17/38] [D loss: 0.3972238004207611] [G loss: 1.8657147884368896]\n",
      "[Epoch 19/40] [Batch 18/38] [D loss: 0.3842025101184845] [G loss: 1.129014015197754]\n",
      "[Epoch 19/40] [Batch 19/38] [D loss: 0.3675971031188965] [G loss: 1.4562790393829346]\n",
      "[Epoch 19/40] [Batch 20/38] [D loss: 0.4696183204650879] [G loss: 1.4270344972610474]\n",
      "[Epoch 19/40] [Batch 21/38] [D loss: 0.5429158210754395] [G loss: 1.1282970905303955]\n",
      "[Epoch 19/40] [Batch 22/38] [D loss: 0.531367301940918] [G loss: 1.4584450721740723]\n",
      "[Epoch 19/40] [Batch 23/38] [D loss: 0.49842405319213867] [G loss: 1.2277473211288452]\n",
      "[Epoch 19/40] [Batch 24/38] [D loss: 0.3977310061454773] [G loss: 1.0984201431274414]\n",
      "[Epoch 19/40] [Batch 25/38] [D loss: 0.41479039192199707] [G loss: 1.5000873804092407]\n",
      "[Epoch 19/40] [Batch 26/38] [D loss: 0.4148656725883484] [G loss: 1.1255673170089722]\n",
      "[Epoch 19/40] [Batch 27/38] [D loss: 0.5034987926483154] [G loss: 0.9464673399925232]\n",
      "[Epoch 19/40] [Batch 28/38] [D loss: 0.4863055646419525] [G loss: 1.4432151317596436]\n",
      "[Epoch 19/40] [Batch 29/38] [D loss: 0.5118142366409302] [G loss: 1.158135175704956]\n",
      "[Epoch 19/40] [Batch 30/38] [D loss: 0.5331947207450867] [G loss: 1.008980393409729]\n",
      "[Epoch 19/40] [Batch 31/38] [D loss: 0.37530577182769775] [G loss: 1.5100092887878418]\n",
      "[Epoch 19/40] [Batch 32/38] [D loss: 0.48576807975769043] [G loss: 1.375819444656372]\n",
      "[Epoch 19/40] [Batch 33/38] [D loss: 0.40596556663513184] [G loss: 1.1284308433532715]\n",
      "[Epoch 19/40] [Batch 34/38] [D loss: 0.43724751472473145] [G loss: 1.4543718099594116]\n",
      "[Epoch 19/40] [Batch 35/38] [D loss: 0.4424343705177307] [G loss: 1.2074775695800781]\n",
      "[Epoch 19/40] [Batch 36/38] [D loss: 0.4739508032798767] [G loss: 1.5638434886932373]\n",
      "[Epoch 19/40] [Batch 37/38] [D loss: 0.35371333360671997] [G loss: 1.170549988746643]\n",
      "[Epoch 20/40] [Batch 0/38] [D loss: 0.45469874143600464] [G loss: 0.9258276224136353]\n",
      "[Epoch 20/40] [Batch 1/38] [D loss: 0.42759448289871216] [G loss: 1.9040213823318481]\n",
      "[Epoch 20/40] [Batch 2/38] [D loss: 0.4014643430709839] [G loss: 1.5492539405822754]\n",
      "[Epoch 20/40] [Batch 3/38] [D loss: 0.5408320426940918] [G loss: 0.874660074710846]\n",
      "[Epoch 20/40] [Batch 4/38] [D loss: 0.5211102962493896] [G loss: 0.9105430245399475]\n",
      "[Epoch 20/40] [Batch 5/38] [D loss: 0.5604016184806824] [G loss: 1.7923974990844727]\n",
      "[Epoch 20/40] [Batch 6/38] [D loss: 0.5244558453559875] [G loss: 1.1956311464309692]\n",
      "[Epoch 20/40] [Batch 7/38] [D loss: 0.5160220861434937] [G loss: 1.2032487392425537]\n",
      "[Epoch 20/40] [Batch 8/38] [D loss: 0.4680233895778656] [G loss: 1.3782533407211304]\n",
      "[Epoch 20/40] [Batch 9/38] [D loss: 0.5163993835449219] [G loss: 1.1684600114822388]\n",
      "[Epoch 20/40] [Batch 10/38] [D loss: 0.3995090126991272] [G loss: 0.9985385537147522]\n",
      "[Epoch 20/40] [Batch 11/38] [D loss: 0.4512608051300049] [G loss: 1.8121483325958252]\n",
      "[Epoch 20/40] [Batch 12/38] [D loss: 0.3672202229499817] [G loss: 1.2561091184616089]\n",
      "[Epoch 20/40] [Batch 13/38] [D loss: 0.4803565740585327] [G loss: 1.0565730333328247]\n",
      "[Epoch 20/40] [Batch 14/38] [D loss: 0.565868079662323] [G loss: 1.4440447092056274]\n",
      "[Epoch 20/40] [Batch 15/38] [D loss: 0.4932204484939575] [G loss: 1.1167826652526855]\n",
      "[Epoch 20/40] [Batch 16/38] [D loss: 0.5387920141220093] [G loss: 1.369248867034912]\n",
      "[Epoch 20/40] [Batch 17/38] [D loss: 0.4684787690639496] [G loss: 1.5724338293075562]\n",
      "[Epoch 20/40] [Batch 18/38] [D loss: 0.3844395577907562] [G loss: 1.246909499168396]\n",
      "[Epoch 20/40] [Batch 19/38] [D loss: 0.5232022404670715] [G loss: 1.547070860862732]\n",
      "[Epoch 20/40] [Batch 20/38] [D loss: 0.41534632444381714] [G loss: 1.3622379302978516]\n",
      "[Epoch 20/40] [Batch 21/38] [D loss: 0.7320408225059509] [G loss: 1.0321789979934692]\n",
      "[Epoch 20/40] [Batch 22/38] [D loss: 0.5129344463348389] [G loss: 1.116689920425415]\n",
      "[Epoch 20/40] [Batch 23/38] [D loss: 0.4221287965774536] [G loss: 2.03251576423645]\n",
      "[Epoch 20/40] [Batch 24/38] [D loss: 0.48950672149658203] [G loss: 1.3612490892410278]\n",
      "[Epoch 20/40] [Batch 25/38] [D loss: 0.4109320044517517] [G loss: 1.132365345954895]\n",
      "[Epoch 20/40] [Batch 26/38] [D loss: 0.3148816227912903] [G loss: 2.021345376968384]\n",
      "[Epoch 20/40] [Batch 27/38] [D loss: 0.36661335825920105] [G loss: 1.831005334854126]\n",
      "[Epoch 20/40] [Batch 28/38] [D loss: 0.2663158178329468] [G loss: 1.893139123916626]\n",
      "[Epoch 20/40] [Batch 29/38] [D loss: 0.38472557067871094] [G loss: 1.733581304550171]\n",
      "[Epoch 20/40] [Batch 30/38] [D loss: 0.47798556089401245] [G loss: 1.666045069694519]\n",
      "[Epoch 20/40] [Batch 31/38] [D loss: 0.6747639775276184] [G loss: 0.8592360615730286]\n",
      "[Epoch 20/40] [Batch 32/38] [D loss: 0.4988190829753876] [G loss: 1.1930080652236938]\n",
      "[Epoch 20/40] [Batch 33/38] [D loss: 0.716858983039856] [G loss: 1.0676491260528564]\n",
      "[Epoch 20/40] [Batch 34/38] [D loss: 0.7372730374336243] [G loss: 0.9773262739181519]\n",
      "[Epoch 20/40] [Batch 35/38] [D loss: 0.5007892847061157] [G loss: 1.1390025615692139]\n",
      "[Epoch 20/40] [Batch 36/38] [D loss: 0.4004742503166199] [G loss: 1.7795313596725464]\n",
      "[Epoch 20/40] [Batch 37/38] [D loss: 0.37344780564308167] [G loss: 1.3267920017242432]\n",
      "[Epoch 21/40] [Batch 0/38] [D loss: 0.41580653190612793] [G loss: 1.2319962978363037]\n",
      "[Epoch 21/40] [Batch 1/38] [D loss: 0.5381801128387451] [G loss: 1.4502239227294922]\n",
      "[Epoch 21/40] [Batch 2/38] [D loss: 0.525449812412262] [G loss: 1.0300815105438232]\n",
      "[Epoch 21/40] [Batch 3/38] [D loss: 0.459795743227005] [G loss: 1.1355594396591187]\n",
      "[Epoch 21/40] [Batch 4/38] [D loss: 0.5366377830505371] [G loss: 1.1358883380889893]\n",
      "[Epoch 21/40] [Batch 5/38] [D loss: 0.6760638356208801] [G loss: 1.2456345558166504]\n",
      "[Epoch 21/40] [Batch 6/38] [D loss: 0.5733509063720703] [G loss: 1.660509467124939]\n",
      "[Epoch 21/40] [Batch 7/38] [D loss: 0.3182885944843292] [G loss: 1.3419572114944458]\n",
      "[Epoch 21/40] [Batch 8/38] [D loss: 0.3156295418739319] [G loss: 1.8855844736099243]\n",
      "[Epoch 21/40] [Batch 9/38] [D loss: 0.35631808638572693] [G loss: 1.7966268062591553]\n",
      "[Epoch 21/40] [Batch 10/38] [D loss: 0.39592665433883667] [G loss: 1.3128271102905273]\n",
      "[Epoch 21/40] [Batch 11/38] [D loss: 0.40939944982528687] [G loss: 1.8134326934814453]\n",
      "[Epoch 21/40] [Batch 12/38] [D loss: 0.34323349595069885] [G loss: 1.1372612714767456]\n",
      "[Epoch 21/40] [Batch 13/38] [D loss: 0.48689404129981995] [G loss: 1.685333251953125]\n",
      "[Epoch 21/40] [Batch 14/38] [D loss: 0.49168872833251953] [G loss: 1.1420292854309082]\n",
      "[Epoch 21/40] [Batch 15/38] [D loss: 0.46504777669906616] [G loss: 1.05618417263031]\n",
      "[Epoch 21/40] [Batch 16/38] [D loss: 0.5055042505264282] [G loss: 1.440114140510559]\n",
      "[Epoch 21/40] [Batch 17/38] [D loss: 0.44806963205337524] [G loss: 1.593016266822815]\n",
      "[Epoch 21/40] [Batch 18/38] [D loss: 0.4675372838973999] [G loss: 1.5990071296691895]\n",
      "[Epoch 21/40] [Batch 19/38] [D loss: 0.46650755405426025] [G loss: 1.06032395362854]\n",
      "[Epoch 21/40] [Batch 20/38] [D loss: 0.41389358043670654] [G loss: 1.680783748626709]\n",
      "[Epoch 21/40] [Batch 21/38] [D loss: 0.41437584161758423] [G loss: 1.543986439704895]\n",
      "[Epoch 21/40] [Batch 22/38] [D loss: 0.4609469175338745] [G loss: 1.6023249626159668]\n",
      "[Epoch 21/40] [Batch 23/38] [D loss: 0.5700360536575317] [G loss: 0.6352829337120056]\n",
      "[Epoch 21/40] [Batch 24/38] [D loss: 0.4425128698348999] [G loss: 0.9553215503692627]\n",
      "[Epoch 21/40] [Batch 25/38] [D loss: 0.6429687738418579] [G loss: 2.0529041290283203]\n",
      "[Epoch 21/40] [Batch 26/38] [D loss: 0.444358229637146] [G loss: 1.0918121337890625]\n",
      "[Epoch 21/40] [Batch 27/38] [D loss: 0.4926907420158386] [G loss: 1.3109400272369385]\n",
      "[Epoch 21/40] [Batch 28/38] [D loss: 0.3757530450820923] [G loss: 1.3056914806365967]\n",
      "[Epoch 21/40] [Batch 29/38] [D loss: 0.4472801089286804] [G loss: 1.0516536235809326]\n",
      "[Epoch 21/40] [Batch 30/38] [D loss: 0.3397762179374695] [G loss: 1.6954998970031738]\n",
      "[Epoch 21/40] [Batch 31/38] [D loss: 0.4408288300037384] [G loss: 1.3958128690719604]\n",
      "[Epoch 21/40] [Batch 32/38] [D loss: 0.3886723220348358] [G loss: 1.470045804977417]\n",
      "[Epoch 21/40] [Batch 33/38] [D loss: 0.5129271149635315] [G loss: 1.0288493633270264]\n",
      "[Epoch 21/40] [Batch 34/38] [D loss: 0.4811461865901947] [G loss: 1.8294093608856201]\n",
      "[Epoch 21/40] [Batch 35/38] [D loss: 0.4577557444572449] [G loss: 1.617043137550354]\n",
      "[Epoch 21/40] [Batch 36/38] [D loss: 0.4932321310043335] [G loss: 0.7998907566070557]\n",
      "[Epoch 21/40] [Batch 37/38] [D loss: 0.34876489639282227] [G loss: 2.013613224029541]\n",
      "[Epoch 22/40] [Batch 0/38] [D loss: 0.42626604437828064] [G loss: 1.3953557014465332]\n",
      "[Epoch 22/40] [Batch 1/38] [D loss: 0.4463217854499817] [G loss: 1.1654541492462158]\n",
      "[Epoch 22/40] [Batch 2/38] [D loss: 0.42780056595802307] [G loss: 1.742979884147644]\n",
      "[Epoch 22/40] [Batch 3/38] [D loss: 0.531341016292572] [G loss: 1.193642020225525]\n",
      "[Epoch 22/40] [Batch 4/38] [D loss: 0.3787270784378052] [G loss: 1.4541386365890503]\n",
      "[Epoch 22/40] [Batch 5/38] [D loss: 0.32762521505355835] [G loss: 2.0760843753814697]\n",
      "[Epoch 22/40] [Batch 6/38] [D loss: 0.32171371579170227] [G loss: 1.7295281887054443]\n",
      "[Epoch 22/40] [Batch 7/38] [D loss: 0.2168249487876892] [G loss: 2.177671432495117]\n",
      "[Epoch 22/40] [Batch 8/38] [D loss: 0.2965213656425476] [G loss: 2.008129835128784]\n",
      "[Epoch 22/40] [Batch 9/38] [D loss: 0.29293036460876465] [G loss: 1.8137845993041992]\n",
      "[Epoch 22/40] [Batch 10/38] [D loss: 0.48559221625328064] [G loss: 1.106550931930542]\n",
      "[Epoch 22/40] [Batch 11/38] [D loss: 0.5537548065185547] [G loss: 1.3669801950454712]\n",
      "[Epoch 22/40] [Batch 12/38] [D loss: 0.5119651556015015] [G loss: 1.1466175317764282]\n",
      "[Epoch 22/40] [Batch 13/38] [D loss: 0.3778277039527893] [G loss: 1.1925400495529175]\n",
      "[Epoch 22/40] [Batch 14/38] [D loss: 0.4090997576713562] [G loss: 1.1945539712905884]\n",
      "[Epoch 22/40] [Batch 15/38] [D loss: 0.4021296501159668] [G loss: 2.0176568031311035]\n",
      "[Epoch 22/40] [Batch 16/38] [D loss: 0.3080112934112549] [G loss: 1.5437644720077515]\n",
      "[Epoch 22/40] [Batch 17/38] [D loss: 0.34440726041793823] [G loss: 1.609702229499817]\n",
      "[Epoch 22/40] [Batch 18/38] [D loss: 0.36959242820739746] [G loss: 1.3550608158111572]\n",
      "[Epoch 22/40] [Batch 19/38] [D loss: 0.5086684226989746] [G loss: 1.6500487327575684]\n",
      "[Epoch 22/40] [Batch 20/38] [D loss: 0.4897134304046631] [G loss: 1.5342258214950562]\n",
      "[Epoch 22/40] [Batch 21/38] [D loss: 0.44921016693115234] [G loss: 1.3195905685424805]\n",
      "[Epoch 22/40] [Batch 22/38] [D loss: 0.4510388970375061] [G loss: 1.6701130867004395]\n",
      "[Epoch 22/40] [Batch 23/38] [D loss: 0.3736962378025055] [G loss: 1.426208734512329]\n",
      "[Epoch 22/40] [Batch 24/38] [D loss: 0.4942447543144226] [G loss: 1.1124944686889648]\n",
      "[Epoch 22/40] [Batch 25/38] [D loss: 0.431963175535202] [G loss: 1.6194782257080078]\n",
      "[Epoch 22/40] [Batch 26/38] [D loss: 0.3857448995113373] [G loss: 1.253503680229187]\n",
      "[Epoch 22/40] [Batch 27/38] [D loss: 0.5161474943161011] [G loss: 1.0419797897338867]\n",
      "[Epoch 22/40] [Batch 28/38] [D loss: 0.6091307997703552] [G loss: 1.1427485942840576]\n",
      "[Epoch 22/40] [Batch 29/38] [D loss: 0.48366492986679077] [G loss: 1.1784169673919678]\n",
      "[Epoch 22/40] [Batch 30/38] [D loss: 0.38388216495513916] [G loss: 1.1103357076644897]\n",
      "[Epoch 22/40] [Batch 31/38] [D loss: 0.4459766745567322] [G loss: 1.345702886581421]\n",
      "[Epoch 22/40] [Batch 32/38] [D loss: 0.5794672966003418] [G loss: 1.29856538772583]\n",
      "[Epoch 22/40] [Batch 33/38] [D loss: 0.552221417427063] [G loss: 0.999354362487793]\n",
      "[Epoch 22/40] [Batch 34/38] [D loss: 0.4279090166091919] [G loss: 1.3158516883850098]\n",
      "[Epoch 22/40] [Batch 35/38] [D loss: 0.5798373222351074] [G loss: 1.09980046749115]\n",
      "[Epoch 22/40] [Batch 36/38] [D loss: 0.5108593106269836] [G loss: 1.1262156963348389]\n",
      "[Epoch 22/40] [Batch 37/38] [D loss: 0.9650077819824219] [G loss: 1.550421953201294]\n",
      "[Epoch 23/40] [Batch 0/38] [D loss: 0.6495137214660645] [G loss: 0.6071256995201111]\n",
      "[Epoch 23/40] [Batch 1/38] [D loss: 0.642595648765564] [G loss: 0.8875896334648132]\n",
      "[Epoch 23/40] [Batch 2/38] [D loss: 0.7259206175804138] [G loss: 1.7123911380767822]\n",
      "[Epoch 23/40] [Batch 3/38] [D loss: 0.5492537021636963] [G loss: 0.8700827956199646]\n",
      "[Epoch 23/40] [Batch 4/38] [D loss: 0.5017061829566956] [G loss: 1.5075464248657227]\n",
      "[Epoch 23/40] [Batch 5/38] [D loss: 0.4510752558708191] [G loss: 1.1315889358520508]\n",
      "[Epoch 23/40] [Batch 6/38] [D loss: 0.4971395432949066] [G loss: 1.5509603023529053]\n",
      "[Epoch 23/40] [Batch 7/38] [D loss: 0.5013062953948975] [G loss: 1.3263702392578125]\n",
      "[Epoch 23/40] [Batch 8/38] [D loss: 0.3710477948188782] [G loss: 1.3689700365066528]\n",
      "[Epoch 23/40] [Batch 9/38] [D loss: 0.416953444480896] [G loss: 1.5956664085388184]\n",
      "[Epoch 23/40] [Batch 10/38] [D loss: 0.39980050921440125] [G loss: 1.2815462350845337]\n",
      "[Epoch 23/40] [Batch 11/38] [D loss: 0.5417290329933167] [G loss: 1.397475242614746]\n",
      "[Epoch 23/40] [Batch 12/38] [D loss: 0.45871061086654663] [G loss: 1.834613561630249]\n",
      "[Epoch 23/40] [Batch 13/38] [D loss: 0.5177847743034363] [G loss: 1.0559542179107666]\n",
      "[Epoch 23/40] [Batch 14/38] [D loss: 0.5485265851020813] [G loss: 0.8753930926322937]\n",
      "[Epoch 23/40] [Batch 15/38] [D loss: 0.4400668144226074] [G loss: 1.5409306287765503]\n",
      "[Epoch 23/40] [Batch 16/38] [D loss: 0.4900486171245575] [G loss: 1.598556399345398]\n",
      "[Epoch 23/40] [Batch 17/38] [D loss: 0.47087734937667847] [G loss: 1.2035706043243408]\n",
      "[Epoch 23/40] [Batch 18/38] [D loss: 0.4114621877670288] [G loss: 1.2900792360305786]\n",
      "[Epoch 23/40] [Batch 19/38] [D loss: 0.3960148096084595] [G loss: 1.80356764793396]\n",
      "[Epoch 23/40] [Batch 20/38] [D loss: 0.544041097164154] [G loss: 0.9286898970603943]\n",
      "[Epoch 23/40] [Batch 21/38] [D loss: 0.5019387006759644] [G loss: 1.1448795795440674]\n",
      "[Epoch 23/40] [Batch 22/38] [D loss: 0.4567095935344696] [G loss: 1.3693770170211792]\n",
      "[Epoch 23/40] [Batch 23/38] [D loss: 0.45020073652267456] [G loss: 1.4738450050354004]\n",
      "[Epoch 23/40] [Batch 24/38] [D loss: 0.38210123777389526] [G loss: 1.7049272060394287]\n",
      "[Epoch 23/40] [Batch 25/38] [D loss: 0.46079903841018677] [G loss: 1.2094449996948242]\n",
      "[Epoch 23/40] [Batch 26/38] [D loss: 0.5236607193946838] [G loss: 0.7012858390808105]\n",
      "[Epoch 23/40] [Batch 27/38] [D loss: 0.38307321071624756] [G loss: 1.8546175956726074]\n",
      "[Epoch 23/40] [Batch 28/38] [D loss: 0.5284931659698486] [G loss: 1.4867388010025024]\n",
      "[Epoch 23/40] [Batch 29/38] [D loss: 0.47912484407424927] [G loss: 1.1840373277664185]\n",
      "[Epoch 23/40] [Batch 30/38] [D loss: 0.4235323667526245] [G loss: 1.4833065271377563]\n",
      "[Epoch 23/40] [Batch 31/38] [D loss: 0.4520883560180664] [G loss: 1.2509372234344482]\n",
      "[Epoch 23/40] [Batch 32/38] [D loss: 0.41664817929267883] [G loss: 1.8953288793563843]\n",
      "[Epoch 23/40] [Batch 33/38] [D loss: 0.4790354073047638] [G loss: 1.4879459142684937]\n",
      "[Epoch 23/40] [Batch 34/38] [D loss: 0.5918725728988647] [G loss: 1.356541395187378]\n",
      "[Epoch 23/40] [Batch 35/38] [D loss: 0.5462429523468018] [G loss: 1.461313009262085]\n",
      "[Epoch 23/40] [Batch 36/38] [D loss: 0.6216729879379272] [G loss: 0.9810022115707397]\n",
      "[Epoch 23/40] [Batch 37/38] [D loss: 0.5780318379402161] [G loss: 1.5720545053482056]\n",
      "[Epoch 24/40] [Batch 0/38] [D loss: 0.5700514316558838] [G loss: 0.6893892288208008]\n",
      "[Epoch 24/40] [Batch 1/38] [D loss: 0.48939335346221924] [G loss: 1.7552945613861084]\n",
      "[Epoch 24/40] [Batch 2/38] [D loss: 0.46252042055130005] [G loss: 2.034611463546753]\n",
      "[Epoch 24/40] [Batch 3/38] [D loss: 0.6277191638946533] [G loss: 0.8718026876449585]\n",
      "[Epoch 24/40] [Batch 4/38] [D loss: 0.4656091332435608] [G loss: 1.3668148517608643]\n",
      "[Epoch 24/40] [Batch 5/38] [D loss: 0.4812207520008087] [G loss: 1.3797171115875244]\n",
      "[Epoch 24/40] [Batch 6/38] [D loss: 0.49748694896698] [G loss: 0.7449512481689453]\n",
      "[Epoch 24/40] [Batch 7/38] [D loss: 0.4167819619178772] [G loss: 1.8368208408355713]\n",
      "[Epoch 24/40] [Batch 8/38] [D loss: 0.4910566210746765] [G loss: 1.621163010597229]\n",
      "[Epoch 24/40] [Batch 9/38] [D loss: 0.8253419995307922] [G loss: 0.48672449588775635]\n",
      "[Epoch 24/40] [Batch 10/38] [D loss: 0.5378320217132568] [G loss: 1.4178403615951538]\n",
      "[Epoch 24/40] [Batch 11/38] [D loss: 0.49814507365226746] [G loss: 1.7410379648208618]\n",
      "[Epoch 24/40] [Batch 12/38] [D loss: 0.5677123665809631] [G loss: 0.6928989291191101]\n",
      "[Epoch 24/40] [Batch 13/38] [D loss: 0.500247597694397] [G loss: 1.1257623434066772]\n",
      "[Epoch 24/40] [Batch 14/38] [D loss: 0.4927537441253662] [G loss: 1.3910326957702637]\n",
      "[Epoch 24/40] [Batch 15/38] [D loss: 0.5516700744628906] [G loss: 0.8655281066894531]\n",
      "[Epoch 24/40] [Batch 16/38] [D loss: 0.4867619276046753] [G loss: 1.358386516571045]\n",
      "[Epoch 24/40] [Batch 17/38] [D loss: 0.5252289772033691] [G loss: 1.4056494235992432]\n",
      "[Epoch 24/40] [Batch 18/38] [D loss: 0.5703912973403931] [G loss: 1.0664148330688477]\n",
      "[Epoch 24/40] [Batch 19/38] [D loss: 0.5543695688247681] [G loss: 1.454366683959961]\n",
      "[Epoch 24/40] [Batch 20/38] [D loss: 0.44999516010284424] [G loss: 1.49897038936615]\n",
      "[Epoch 24/40] [Batch 21/38] [D loss: 0.4320569634437561] [G loss: 1.054541826248169]\n",
      "[Epoch 24/40] [Batch 22/38] [D loss: 0.5538849234580994] [G loss: 1.0655829906463623]\n",
      "[Epoch 24/40] [Batch 23/38] [D loss: 0.49180662631988525] [G loss: 1.3824071884155273]\n",
      "[Epoch 24/40] [Batch 24/38] [D loss: 0.5786658525466919] [G loss: 1.095468521118164]\n",
      "[Epoch 24/40] [Batch 25/38] [D loss: 0.7332732677459717] [G loss: 1.1174660921096802]\n",
      "[Epoch 24/40] [Batch 26/38] [D loss: 0.4808090329170227] [G loss: 1.6044812202453613]\n",
      "[Epoch 24/40] [Batch 27/38] [D loss: 0.41154325008392334] [G loss: 1.444685697555542]\n",
      "[Epoch 24/40] [Batch 28/38] [D loss: 0.5804336667060852] [G loss: 0.960472047328949]\n",
      "[Epoch 24/40] [Batch 29/38] [D loss: 0.6342580914497375] [G loss: 0.9466264247894287]\n",
      "[Epoch 24/40] [Batch 30/38] [D loss: 0.4752409756183624] [G loss: 1.716046929359436]\n",
      "[Epoch 24/40] [Batch 31/38] [D loss: 0.5451244115829468] [G loss: 0.949284553527832]\n",
      "[Epoch 24/40] [Batch 32/38] [D loss: 0.5334520936012268] [G loss: 1.9589515924453735]\n",
      "[Epoch 24/40] [Batch 33/38] [D loss: 0.48886874318122864] [G loss: 1.2379066944122314]\n",
      "[Epoch 24/40] [Batch 34/38] [D loss: 0.5237730145454407] [G loss: 1.161524772644043]\n",
      "[Epoch 24/40] [Batch 35/38] [D loss: 0.5706145763397217] [G loss: 1.2179009914398193]\n",
      "[Epoch 24/40] [Batch 36/38] [D loss: 0.5561238527297974] [G loss: 1.4260072708129883]\n",
      "[Epoch 24/40] [Batch 37/38] [D loss: 0.6523250937461853] [G loss: 1.4908705949783325]\n",
      "[Epoch 25/40] [Batch 0/38] [D loss: 0.6299676895141602] [G loss: 0.6785072684288025]\n",
      "[Epoch 25/40] [Batch 1/38] [D loss: 0.44719743728637695] [G loss: 1.4940719604492188]\n",
      "[Epoch 25/40] [Batch 2/38] [D loss: 0.36748450994491577] [G loss: 1.711905837059021]\n",
      "[Epoch 25/40] [Batch 3/38] [D loss: 0.3712587356567383] [G loss: 1.4863876104354858]\n",
      "[Epoch 25/40] [Batch 4/38] [D loss: 0.5905790328979492] [G loss: 0.7599583864212036]\n",
      "[Epoch 25/40] [Batch 5/38] [D loss: 0.8095434308052063] [G loss: 1.5663912296295166]\n",
      "[Epoch 25/40] [Batch 6/38] [D loss: 0.636589527130127] [G loss: 0.6819555759429932]\n",
      "[Epoch 25/40] [Batch 7/38] [D loss: 0.5998943448066711] [G loss: 0.9657647013664246]\n",
      "[Epoch 25/40] [Batch 8/38] [D loss: 0.4837226867675781] [G loss: 1.4282588958740234]\n",
      "[Epoch 25/40] [Batch 9/38] [D loss: 0.5473833084106445] [G loss: 1.288439393043518]\n",
      "[Epoch 25/40] [Batch 10/38] [D loss: 0.4518888592720032] [G loss: 1.7250616550445557]\n",
      "[Epoch 25/40] [Batch 11/38] [D loss: 0.429431676864624] [G loss: 1.51322603225708]\n",
      "[Epoch 25/40] [Batch 12/38] [D loss: 0.4273403584957123] [G loss: 1.1018376350402832]\n",
      "[Epoch 25/40] [Batch 13/38] [D loss: 0.6430515050888062] [G loss: 1.2393854856491089]\n",
      "[Epoch 25/40] [Batch 14/38] [D loss: 0.35558372735977173] [G loss: 1.0833176374435425]\n",
      "[Epoch 25/40] [Batch 15/38] [D loss: 0.46295005083084106] [G loss: 1.5300829410552979]\n",
      "[Epoch 25/40] [Batch 16/38] [D loss: 0.5987714529037476] [G loss: 1.1780937910079956]\n",
      "[Epoch 25/40] [Batch 17/38] [D loss: 0.5452139377593994] [G loss: 1.1421582698822021]\n",
      "[Epoch 25/40] [Batch 18/38] [D loss: 0.4833105206489563] [G loss: 1.818852424621582]\n",
      "[Epoch 25/40] [Batch 19/38] [D loss: 0.5677387714385986] [G loss: 1.0479655265808105]\n",
      "[Epoch 25/40] [Batch 20/38] [D loss: 0.6311795115470886] [G loss: 1.9238576889038086]\n",
      "[Epoch 25/40] [Batch 21/38] [D loss: 0.5944017767906189] [G loss: 0.8371357917785645]\n",
      "[Epoch 25/40] [Batch 22/38] [D loss: 0.4757835268974304] [G loss: 1.1070592403411865]\n",
      "[Epoch 25/40] [Batch 23/38] [D loss: 0.45609650015830994] [G loss: 1.9373788833618164]\n",
      "[Epoch 25/40] [Batch 24/38] [D loss: 0.6093372702598572] [G loss: 1.1158363819122314]\n",
      "[Epoch 25/40] [Batch 25/38] [D loss: 0.5432724952697754] [G loss: 1.4569125175476074]\n",
      "[Epoch 25/40] [Batch 26/38] [D loss: 0.5236266851425171] [G loss: 1.2959636449813843]\n",
      "[Epoch 25/40] [Batch 27/38] [D loss: 0.3812868893146515] [G loss: 1.0061707496643066]\n",
      "[Epoch 25/40] [Batch 28/38] [D loss: 0.7545683979988098] [G loss: 1.6513859033584595]\n",
      "[Epoch 25/40] [Batch 29/38] [D loss: 0.5396798849105835] [G loss: 0.7925577759742737]\n",
      "[Epoch 25/40] [Batch 30/38] [D loss: 0.4585811495780945] [G loss: 1.6918896436691284]\n",
      "[Epoch 25/40] [Batch 31/38] [D loss: 0.33640360832214355] [G loss: 1.4998328685760498]\n",
      "[Epoch 25/40] [Batch 32/38] [D loss: 0.4409288465976715] [G loss: 1.3544214963912964]\n",
      "[Epoch 25/40] [Batch 33/38] [D loss: 0.5795284509658813] [G loss: 0.9219539165496826]\n",
      "[Epoch 25/40] [Batch 34/38] [D loss: 0.5840426087379456] [G loss: 1.3436084985733032]\n",
      "[Epoch 25/40] [Batch 35/38] [D loss: 0.5867973566055298] [G loss: 0.967886745929718]\n",
      "[Epoch 25/40] [Batch 36/38] [D loss: 0.6796373724937439] [G loss: 1.2536598443984985]\n",
      "[Epoch 25/40] [Batch 37/38] [D loss: 0.6373703479766846] [G loss: 0.6966165900230408]\n",
      "[Epoch 26/40] [Batch 0/38] [D loss: 0.45776331424713135] [G loss: 1.1150330305099487]\n",
      "[Epoch 26/40] [Batch 1/38] [D loss: 0.3982740640640259] [G loss: 1.7386986017227173]\n",
      "[Epoch 26/40] [Batch 2/38] [D loss: 0.4469109773635864] [G loss: 1.4476310014724731]\n",
      "[Epoch 26/40] [Batch 3/38] [D loss: 0.5364686250686646] [G loss: 1.314115285873413]\n",
      "[Epoch 26/40] [Batch 4/38] [D loss: 0.7476974129676819] [G loss: 1.2589691877365112]\n",
      "[Epoch 26/40] [Batch 5/38] [D loss: 0.5551938414573669] [G loss: 0.7269637584686279]\n",
      "[Epoch 26/40] [Batch 6/38] [D loss: 0.5939602851867676] [G loss: 1.157924771308899]\n",
      "[Epoch 26/40] [Batch 7/38] [D loss: 0.5536658763885498] [G loss: 1.6579158306121826]\n",
      "[Epoch 26/40] [Batch 8/38] [D loss: 0.7179462909698486] [G loss: 1.0013283491134644]\n",
      "[Epoch 26/40] [Batch 9/38] [D loss: 0.6192687749862671] [G loss: 1.1866483688354492]\n",
      "[Epoch 26/40] [Batch 10/38] [D loss: 0.3921763598918915] [G loss: 1.2734190225601196]\n",
      "[Epoch 26/40] [Batch 11/38] [D loss: 0.43380868434906006] [G loss: 1.5566681623458862]\n",
      "[Epoch 26/40] [Batch 12/38] [D loss: 0.5142539739608765] [G loss: 1.3355129957199097]\n",
      "[Epoch 26/40] [Batch 13/38] [D loss: 0.43054184317588806] [G loss: 1.1615101099014282]\n",
      "[Epoch 26/40] [Batch 14/38] [D loss: 0.35097968578338623] [G loss: 1.4728505611419678]\n",
      "[Epoch 26/40] [Batch 15/38] [D loss: 0.37717485427856445] [G loss: 1.4046084880828857]\n",
      "[Epoch 26/40] [Batch 16/38] [D loss: 0.4487724304199219] [G loss: 1.497687816619873]\n",
      "[Epoch 26/40] [Batch 17/38] [D loss: 0.39958521723747253] [G loss: 0.950480043888092]\n",
      "[Epoch 26/40] [Batch 18/38] [D loss: 0.4588788151741028] [G loss: 1.8716682195663452]\n",
      "[Epoch 26/40] [Batch 19/38] [D loss: 0.43032020330429077] [G loss: 1.6006723642349243]\n",
      "[Epoch 26/40] [Batch 20/38] [D loss: 0.5016404390335083] [G loss: 1.1460503339767456]\n",
      "[Epoch 26/40] [Batch 21/38] [D loss: 0.4746953248977661] [G loss: 1.367842197418213]\n",
      "[Epoch 26/40] [Batch 22/38] [D loss: 0.6352298259735107] [G loss: 1.3687763214111328]\n",
      "[Epoch 26/40] [Batch 23/38] [D loss: 0.38817155361175537] [G loss: 0.9349342584609985]\n",
      "[Epoch 26/40] [Batch 24/38] [D loss: 0.34419509768486023] [G loss: 1.5431926250457764]\n",
      "[Epoch 26/40] [Batch 25/38] [D loss: 0.5282468199729919] [G loss: 2.0287203788757324]\n",
      "[Epoch 26/40] [Batch 26/38] [D loss: 0.5585688352584839] [G loss: 1.1442371606826782]\n",
      "[Epoch 26/40] [Batch 27/38] [D loss: 0.4576871991157532] [G loss: 1.1484824419021606]\n",
      "[Epoch 26/40] [Batch 28/38] [D loss: 0.46812447905540466] [G loss: 2.4052271842956543]\n",
      "[Epoch 26/40] [Batch 29/38] [D loss: 0.34965765476226807] [G loss: 1.3060599565505981]\n",
      "[Epoch 26/40] [Batch 30/38] [D loss: 0.5298168659210205] [G loss: 1.2437125444412231]\n",
      "[Epoch 26/40] [Batch 31/38] [D loss: 0.4347209632396698] [G loss: 1.152958869934082]\n",
      "[Epoch 26/40] [Batch 32/38] [D loss: 0.4674225449562073] [G loss: 1.5627118349075317]\n",
      "[Epoch 26/40] [Batch 33/38] [D loss: 0.46194902062416077] [G loss: 1.168245553970337]\n",
      "[Epoch 26/40] [Batch 34/38] [D loss: 0.48355668783187866] [G loss: 1.4818463325500488]\n",
      "[Epoch 26/40] [Batch 35/38] [D loss: 0.5377109050750732] [G loss: 1.3846184015274048]\n",
      "[Epoch 26/40] [Batch 36/38] [D loss: 0.6475642919540405] [G loss: 1.0992612838745117]\n",
      "[Epoch 26/40] [Batch 37/38] [D loss: 0.5031630992889404] [G loss: 1.34564208984375]\n",
      "[Epoch 27/40] [Batch 0/38] [D loss: 0.4119659960269928] [G loss: 1.4088095426559448]\n",
      "[Epoch 27/40] [Batch 1/38] [D loss: 0.5009435415267944] [G loss: 1.2616385221481323]\n",
      "[Epoch 27/40] [Batch 2/38] [D loss: 0.6439932584762573] [G loss: 1.405465006828308]\n",
      "[Epoch 27/40] [Batch 3/38] [D loss: 0.6126260161399841] [G loss: 1.3559824228286743]\n",
      "[Epoch 27/40] [Batch 4/38] [D loss: 0.6566615104675293] [G loss: 1.0802099704742432]\n",
      "[Epoch 27/40] [Batch 5/38] [D loss: 0.47714030742645264] [G loss: 1.2747422456741333]\n",
      "[Epoch 27/40] [Batch 6/38] [D loss: 0.3888651132583618] [G loss: 1.636047601699829]\n",
      "[Epoch 27/40] [Batch 7/38] [D loss: 0.4492562413215637] [G loss: 1.4748131036758423]\n",
      "[Epoch 27/40] [Batch 8/38] [D loss: 0.48083528876304626] [G loss: 1.1142292022705078]\n",
      "[Epoch 27/40] [Batch 9/38] [D loss: 0.3703906536102295] [G loss: 1.7767093181610107]\n",
      "[Epoch 27/40] [Batch 10/38] [D loss: 0.4946449398994446] [G loss: 1.264266014099121]\n",
      "[Epoch 27/40] [Batch 11/38] [D loss: 0.7008175849914551] [G loss: 0.9229510426521301]\n",
      "[Epoch 27/40] [Batch 12/38] [D loss: 0.6338798403739929] [G loss: 1.707812786102295]\n",
      "[Epoch 27/40] [Batch 13/38] [D loss: 0.49279388785362244] [G loss: 1.6734347343444824]\n",
      "[Epoch 27/40] [Batch 14/38] [D loss: 0.5493343472480774] [G loss: 0.8663341403007507]\n",
      "[Epoch 27/40] [Batch 15/38] [D loss: 0.4076650142669678] [G loss: 0.9231588840484619]\n",
      "[Epoch 27/40] [Batch 16/38] [D loss: 0.3162703514099121] [G loss: 1.5950356721878052]\n",
      "[Epoch 27/40] [Batch 17/38] [D loss: 0.381472647190094] [G loss: 1.2386645078659058]\n",
      "[Epoch 27/40] [Batch 18/38] [D loss: 0.3744315803050995] [G loss: 1.7949999570846558]\n",
      "[Epoch 27/40] [Batch 19/38] [D loss: 0.334647536277771] [G loss: 1.3226652145385742]\n",
      "[Epoch 27/40] [Batch 20/38] [D loss: 0.5326941609382629] [G loss: 1.2972557544708252]\n",
      "[Epoch 27/40] [Batch 21/38] [D loss: 0.4254486858844757] [G loss: 1.0709059238433838]\n",
      "[Epoch 27/40] [Batch 22/38] [D loss: 0.5792409777641296] [G loss: 1.8682960271835327]\n",
      "[Epoch 27/40] [Batch 23/38] [D loss: 0.45514485239982605] [G loss: 1.2037227153778076]\n",
      "[Epoch 27/40] [Batch 24/38] [D loss: 0.6299265027046204] [G loss: 0.9145989418029785]\n",
      "[Epoch 27/40] [Batch 25/38] [D loss: 0.5460216999053955] [G loss: 1.2078242301940918]\n",
      "[Epoch 27/40] [Batch 26/38] [D loss: 0.49666157364845276] [G loss: 1.0911527872085571]\n",
      "[Epoch 27/40] [Batch 27/38] [D loss: 1.0008454322814941] [G loss: 1.1363602876663208]\n",
      "[Epoch 27/40] [Batch 28/38] [D loss: 0.5532324910163879] [G loss: 0.9190123081207275]\n",
      "[Epoch 27/40] [Batch 29/38] [D loss: 0.4913307726383209] [G loss: 1.6579420566558838]\n",
      "[Epoch 27/40] [Batch 30/38] [D loss: 0.4305771589279175] [G loss: 1.2000502347946167]\n",
      "[Epoch 27/40] [Batch 31/38] [D loss: 0.4924317002296448] [G loss: 1.3995704650878906]\n",
      "[Epoch 27/40] [Batch 32/38] [D loss: 0.6233361959457397] [G loss: 1.2111704349517822]\n",
      "[Epoch 27/40] [Batch 33/38] [D loss: 0.533852756023407] [G loss: 0.9363975524902344]\n",
      "[Epoch 27/40] [Batch 34/38] [D loss: 0.5244600772857666] [G loss: 1.6829015016555786]\n",
      "[Epoch 27/40] [Batch 35/38] [D loss: 0.44622164964675903] [G loss: 1.70905339717865]\n",
      "[Epoch 27/40] [Batch 36/38] [D loss: 0.5438932180404663] [G loss: 1.1084004640579224]\n",
      "[Epoch 27/40] [Batch 37/38] [D loss: 0.44921883940696716] [G loss: 1.7142953872680664]\n",
      "[Epoch 28/40] [Batch 0/38] [D loss: 0.43697524070739746] [G loss: 1.108505368232727]\n",
      "[Epoch 28/40] [Batch 1/38] [D loss: 0.394103080034256] [G loss: 2.048438310623169]\n",
      "[Epoch 28/40] [Batch 2/38] [D loss: 0.4803515076637268] [G loss: 1.7602882385253906]\n",
      "[Epoch 28/40] [Batch 3/38] [D loss: 0.6047283411026001] [G loss: 1.0326273441314697]\n",
      "[Epoch 28/40] [Batch 4/38] [D loss: 0.5097041130065918] [G loss: 1.1746116876602173]\n",
      "[Epoch 28/40] [Batch 5/38] [D loss: 0.6070704460144043] [G loss: 1.5276997089385986]\n",
      "[Epoch 28/40] [Batch 6/38] [D loss: 0.5423859357833862] [G loss: 1.5089813470840454]\n",
      "[Epoch 28/40] [Batch 7/38] [D loss: 0.5336251258850098] [G loss: 0.8660563230514526]\n",
      "[Epoch 28/40] [Batch 8/38] [D loss: 0.3775842487812042] [G loss: 1.8299598693847656]\n",
      "[Epoch 28/40] [Batch 9/38] [D loss: 0.44442984461784363] [G loss: 1.3022165298461914]\n",
      "[Epoch 28/40] [Batch 10/38] [D loss: 0.4792275130748749] [G loss: 1.1996536254882812]\n",
      "[Epoch 28/40] [Batch 11/38] [D loss: 0.4305366277694702] [G loss: 0.9982151389122009]\n",
      "[Epoch 28/40] [Batch 12/38] [D loss: 0.37487250566482544] [G loss: 1.8098030090332031]\n",
      "[Epoch 28/40] [Batch 13/38] [D loss: 0.5491586327552795] [G loss: 1.2666617631912231]\n",
      "[Epoch 28/40] [Batch 14/38] [D loss: 0.3137241005897522] [G loss: 1.325535774230957]\n",
      "[Epoch 28/40] [Batch 15/38] [D loss: 0.4892784357070923] [G loss: 1.376091480255127]\n",
      "[Epoch 28/40] [Batch 16/38] [D loss: 0.6905847787857056] [G loss: 0.7846031785011292]\n",
      "[Epoch 28/40] [Batch 17/38] [D loss: 0.5023319721221924] [G loss: 1.2914776802062988]\n",
      "[Epoch 28/40] [Batch 18/38] [D loss: 0.602660059928894] [G loss: 1.3152886629104614]\n",
      "[Epoch 28/40] [Batch 19/38] [D loss: 0.6829599142074585] [G loss: 1.5313459634780884]\n",
      "[Epoch 28/40] [Batch 20/38] [D loss: 0.7442928552627563] [G loss: 0.6324036121368408]\n",
      "[Epoch 28/40] [Batch 21/38] [D loss: 0.5688800811767578] [G loss: 1.516069769859314]\n",
      "[Epoch 28/40] [Batch 22/38] [D loss: 0.5770968198776245] [G loss: 1.4397902488708496]\n",
      "[Epoch 28/40] [Batch 23/38] [D loss: 0.5921006202697754] [G loss: 1.1488699913024902]\n",
      "[Epoch 28/40] [Batch 24/38] [D loss: 0.5287677049636841] [G loss: 1.574615478515625]\n",
      "[Epoch 28/40] [Batch 25/38] [D loss: 0.5218032598495483] [G loss: 1.0952706336975098]\n",
      "[Epoch 28/40] [Batch 26/38] [D loss: 0.5977418422698975] [G loss: 1.268194556236267]\n",
      "[Epoch 28/40] [Batch 27/38] [D loss: 0.5988608002662659] [G loss: 0.9953587651252747]\n",
      "[Epoch 28/40] [Batch 28/38] [D loss: 0.4127698540687561] [G loss: 1.5212196111679077]\n",
      "[Epoch 28/40] [Batch 29/38] [D loss: 0.5325112342834473] [G loss: 1.4004709720611572]\n",
      "[Epoch 28/40] [Batch 30/38] [D loss: 0.5029808878898621] [G loss: 1.0421184301376343]\n",
      "[Epoch 28/40] [Batch 31/38] [D loss: 0.6560497283935547] [G loss: 1.3316364288330078]\n",
      "[Epoch 28/40] [Batch 32/38] [D loss: 0.6085439324378967] [G loss: 1.069590449333191]\n",
      "[Epoch 28/40] [Batch 33/38] [D loss: 0.5483781695365906] [G loss: 0.8277756571769714]\n",
      "[Epoch 28/40] [Batch 34/38] [D loss: 0.4428050220012665] [G loss: 1.54209303855896]\n",
      "[Epoch 28/40] [Batch 35/38] [D loss: 0.35654884576797485] [G loss: 1.7277328968048096]\n",
      "[Epoch 28/40] [Batch 36/38] [D loss: 0.39630836248397827] [G loss: 1.344996690750122]\n",
      "[Epoch 28/40] [Batch 37/38] [D loss: 0.6005643010139465] [G loss: 1.1307493448257446]\n",
      "[Epoch 29/40] [Batch 0/38] [D loss: 0.4159035086631775] [G loss: 2.4323315620422363]\n",
      "[Epoch 29/40] [Batch 1/38] [D loss: 0.35670360922813416] [G loss: 1.1550967693328857]\n",
      "[Epoch 29/40] [Batch 2/38] [D loss: 0.48521584272384644] [G loss: 1.108882188796997]\n",
      "[Epoch 29/40] [Batch 3/38] [D loss: 0.36188453435897827] [G loss: 1.7747503519058228]\n",
      "[Epoch 29/40] [Batch 4/38] [D loss: 0.5906122326850891] [G loss: 1.6333763599395752]\n",
      "[Epoch 29/40] [Batch 5/38] [D loss: 0.6012381315231323] [G loss: 0.9204903244972229]\n",
      "[Epoch 29/40] [Batch 6/38] [D loss: 0.34277382493019104] [G loss: 1.6579631567001343]\n",
      "[Epoch 29/40] [Batch 7/38] [D loss: 0.4438337981700897] [G loss: 1.6023038625717163]\n",
      "[Epoch 29/40] [Batch 8/38] [D loss: 0.6603301763534546] [G loss: 1.0566973686218262]\n",
      "[Epoch 29/40] [Batch 9/38] [D loss: 0.6439609527587891] [G loss: 1.3375810384750366]\n",
      "[Epoch 29/40] [Batch 10/38] [D loss: 0.5252565145492554] [G loss: 1.148514986038208]\n",
      "[Epoch 29/40] [Batch 11/38] [D loss: 0.49199578166007996] [G loss: 1.3241989612579346]\n",
      "[Epoch 29/40] [Batch 12/38] [D loss: 0.40472105145454407] [G loss: 1.3167479038238525]\n",
      "[Epoch 29/40] [Batch 13/38] [D loss: 0.5413272976875305] [G loss: 1.3747892379760742]\n",
      "[Epoch 29/40] [Batch 14/38] [D loss: 0.4080812633037567] [G loss: 1.4102474451065063]\n",
      "[Epoch 29/40] [Batch 15/38] [D loss: 0.5202754735946655] [G loss: 1.2160663604736328]\n",
      "[Epoch 29/40] [Batch 16/38] [D loss: 0.6722266674041748] [G loss: 1.150308609008789]\n",
      "[Epoch 29/40] [Batch 17/38] [D loss: 0.4708651006221771] [G loss: 1.2002501487731934]\n",
      "[Epoch 29/40] [Batch 18/38] [D loss: 0.5186573266983032] [G loss: 0.8973513245582581]\n",
      "[Epoch 29/40] [Batch 19/38] [D loss: 0.5073126554489136] [G loss: 1.1404063701629639]\n",
      "[Epoch 29/40] [Batch 20/38] [D loss: 0.756472110748291] [G loss: 1.258204460144043]\n",
      "[Epoch 29/40] [Batch 21/38] [D loss: 0.6200323104858398] [G loss: 0.8770886063575745]\n",
      "[Epoch 29/40] [Batch 22/38] [D loss: 0.6952224969863892] [G loss: 1.162758708000183]\n",
      "[Epoch 29/40] [Batch 23/38] [D loss: 0.5277455449104309] [G loss: 1.1492332220077515]\n",
      "[Epoch 29/40] [Batch 24/38] [D loss: 0.5599597692489624] [G loss: 1.1029067039489746]\n",
      "[Epoch 29/40] [Batch 25/38] [D loss: 0.4352014660835266] [G loss: 1.350972294807434]\n",
      "[Epoch 29/40] [Batch 26/38] [D loss: 0.5339469909667969] [G loss: 1.22316575050354]\n",
      "[Epoch 29/40] [Batch 27/38] [D loss: 0.5031709671020508] [G loss: 1.0724977254867554]\n",
      "[Epoch 29/40] [Batch 28/38] [D loss: 0.5859116911888123] [G loss: 2.2722461223602295]\n",
      "[Epoch 29/40] [Batch 29/38] [D loss: 0.4950501322746277] [G loss: 0.7783203721046448]\n",
      "[Epoch 29/40] [Batch 30/38] [D loss: 0.5819738507270813] [G loss: 1.3938709497451782]\n",
      "[Epoch 29/40] [Batch 31/38] [D loss: 0.5218925476074219] [G loss: 1.3701649904251099]\n",
      "[Epoch 29/40] [Batch 32/38] [D loss: 0.4400661885738373] [G loss: 1.175079345703125]\n",
      "[Epoch 29/40] [Batch 33/38] [D loss: 0.4019564986228943] [G loss: 1.3326280117034912]\n",
      "[Epoch 29/40] [Batch 34/38] [D loss: 0.3203188180923462] [G loss: 1.60598623752594]\n",
      "[Epoch 29/40] [Batch 35/38] [D loss: 0.5269798040390015] [G loss: 1.528954267501831]\n",
      "[Epoch 29/40] [Batch 36/38] [D loss: 0.467870831489563] [G loss: 1.1096670627593994]\n",
      "[Epoch 29/40] [Batch 37/38] [D loss: 0.4990817606449127] [G loss: 1.5385589599609375]\n",
      "[Epoch 30/40] [Batch 0/38] [D loss: 0.47041401267051697] [G loss: 1.015458106994629]\n",
      "[Epoch 30/40] [Batch 1/38] [D loss: 0.4349430203437805] [G loss: 1.278465986251831]\n",
      "[Epoch 30/40] [Batch 2/38] [D loss: 0.6158350706100464] [G loss: 1.855228066444397]\n",
      "[Epoch 30/40] [Batch 3/38] [D loss: 0.34228047728538513] [G loss: 1.5814576148986816]\n",
      "[Epoch 30/40] [Batch 4/38] [D loss: 0.5470141172409058] [G loss: 1.012363076210022]\n",
      "[Epoch 30/40] [Batch 5/38] [D loss: 0.4717438817024231] [G loss: 0.9770585298538208]\n",
      "[Epoch 30/40] [Batch 6/38] [D loss: 0.44998952746391296] [G loss: 1.9626476764678955]\n",
      "[Epoch 30/40] [Batch 7/38] [D loss: 0.5846570134162903] [G loss: 1.3137933015823364]\n",
      "[Epoch 30/40] [Batch 8/38] [D loss: 0.5304019451141357] [G loss: 0.7449008822441101]\n",
      "[Epoch 30/40] [Batch 9/38] [D loss: 0.45114612579345703] [G loss: 1.5727853775024414]\n",
      "[Epoch 30/40] [Batch 10/38] [D loss: 0.5384160280227661] [G loss: 1.1320841312408447]\n",
      "[Epoch 30/40] [Batch 11/38] [D loss: 0.4654688239097595] [G loss: 0.9714218378067017]\n",
      "[Epoch 30/40] [Batch 12/38] [D loss: 0.4988391399383545] [G loss: 1.2920928001403809]\n",
      "[Epoch 30/40] [Batch 13/38] [D loss: 0.697940468788147] [G loss: 1.5218781232833862]\n",
      "[Epoch 30/40] [Batch 14/38] [D loss: 0.6002157926559448] [G loss: 1.1092647314071655]\n",
      "[Epoch 30/40] [Batch 15/38] [D loss: 0.6152328848838806] [G loss: 1.17470383644104]\n",
      "[Epoch 30/40] [Batch 16/38] [D loss: 0.4891398847103119] [G loss: 1.734100103378296]\n",
      "[Epoch 30/40] [Batch 17/38] [D loss: 0.43623054027557373] [G loss: 1.52983820438385]\n",
      "[Epoch 30/40] [Batch 18/38] [D loss: 0.4807039797306061] [G loss: 1.1307599544525146]\n",
      "[Epoch 30/40] [Batch 19/38] [D loss: 0.40869399905204773] [G loss: 1.1888232231140137]\n",
      "[Epoch 30/40] [Batch 20/38] [D loss: 0.5840392112731934] [G loss: 1.5225110054016113]\n",
      "[Epoch 30/40] [Batch 21/38] [D loss: 0.39276057481765747] [G loss: 1.0329385995864868]\n",
      "[Epoch 30/40] [Batch 22/38] [D loss: 0.5777769088745117] [G loss: 0.9628719091415405]\n",
      "[Epoch 30/40] [Batch 23/38] [D loss: 0.4176347255706787] [G loss: 1.0903005599975586]\n",
      "[Epoch 30/40] [Batch 24/38] [D loss: 0.5333800315856934] [G loss: 1.602059006690979]\n",
      "[Epoch 30/40] [Batch 25/38] [D loss: 0.5262182354927063] [G loss: 0.8742766976356506]\n",
      "[Epoch 30/40] [Batch 26/38] [D loss: 0.5049042105674744] [G loss: 1.013594388961792]\n",
      "[Epoch 30/40] [Batch 27/38] [D loss: 0.6433485746383667] [G loss: 1.6029154062271118]\n",
      "[Epoch 30/40] [Batch 28/38] [D loss: 0.6062289476394653] [G loss: 0.7662757635116577]\n",
      "[Epoch 30/40] [Batch 29/38] [D loss: 0.44074302911758423] [G loss: 1.3617836236953735]\n",
      "[Epoch 30/40] [Batch 30/38] [D loss: 0.4121112525463104] [G loss: 1.5605751276016235]\n",
      "[Epoch 30/40] [Batch 31/38] [D loss: 0.5533765554428101] [G loss: 1.4941951036453247]\n",
      "[Epoch 30/40] [Batch 32/38] [D loss: 0.4859863221645355] [G loss: 1.3322869539260864]\n",
      "[Epoch 30/40] [Batch 33/38] [D loss: 0.5181329250335693] [G loss: 1.248762845993042]\n",
      "[Epoch 30/40] [Batch 34/38] [D loss: 0.7804529666900635] [G loss: 0.7535779476165771]\n",
      "[Epoch 30/40] [Batch 35/38] [D loss: 0.410066694021225] [G loss: 1.4575246572494507]\n",
      "[Epoch 30/40] [Batch 36/38] [D loss: 0.46341344714164734] [G loss: 1.543888807296753]\n",
      "[Epoch 30/40] [Batch 37/38] [D loss: 0.5412603616714478] [G loss: 0.9068819284439087]\n",
      "[Epoch 31/40] [Batch 0/38] [D loss: 0.4477934241294861] [G loss: 1.3359992504119873]\n",
      "[Epoch 31/40] [Batch 1/38] [D loss: 0.47833114862442017] [G loss: 0.8813834190368652]\n",
      "[Epoch 31/40] [Batch 2/38] [D loss: 0.5093665719032288] [G loss: 1.687973141670227]\n",
      "[Epoch 31/40] [Batch 3/38] [D loss: 0.5239261388778687] [G loss: 0.8314250707626343]\n",
      "[Epoch 31/40] [Batch 4/38] [D loss: 0.5067651867866516] [G loss: 1.7582788467407227]\n",
      "[Epoch 31/40] [Batch 5/38] [D loss: 0.43129581212997437] [G loss: 1.2252862453460693]\n",
      "[Epoch 31/40] [Batch 6/38] [D loss: 0.542412281036377] [G loss: 1.3687316179275513]\n",
      "[Epoch 31/40] [Batch 7/38] [D loss: 0.49809712171554565] [G loss: 0.9687862992286682]\n",
      "[Epoch 31/40] [Batch 8/38] [D loss: 0.5112658739089966] [G loss: 2.048964262008667]\n",
      "[Epoch 31/40] [Batch 9/38] [D loss: 0.5387778282165527] [G loss: 0.9827812910079956]\n",
      "[Epoch 31/40] [Batch 10/38] [D loss: 0.60896897315979] [G loss: 0.9524130821228027]\n",
      "[Epoch 31/40] [Batch 11/38] [D loss: 0.7064585089683533] [G loss: 1.4080671072006226]\n",
      "[Epoch 31/40] [Batch 12/38] [D loss: 0.48765188455581665] [G loss: 1.045253038406372]\n",
      "[Epoch 31/40] [Batch 13/38] [D loss: 0.37688252329826355] [G loss: 1.3518930673599243]\n",
      "[Epoch 31/40] [Batch 14/38] [D loss: 0.6110391616821289] [G loss: 1.1542561054229736]\n",
      "[Epoch 31/40] [Batch 15/38] [D loss: 0.48851311206817627] [G loss: 1.5557994842529297]\n",
      "[Epoch 31/40] [Batch 16/38] [D loss: 0.4750998616218567] [G loss: 1.0301467180252075]\n",
      "[Epoch 31/40] [Batch 17/38] [D loss: 0.4660170078277588] [G loss: 1.8454666137695312]\n",
      "[Epoch 31/40] [Batch 18/38] [D loss: 0.3958682715892792] [G loss: 1.7240819931030273]\n",
      "[Epoch 31/40] [Batch 19/38] [D loss: 0.5528092980384827] [G loss: 1.0518800020217896]\n",
      "[Epoch 31/40] [Batch 20/38] [D loss: 0.4681349992752075] [G loss: 1.3226326704025269]\n",
      "[Epoch 31/40] [Batch 21/38] [D loss: 0.49971166253089905] [G loss: 1.5821367502212524]\n",
      "[Epoch 31/40] [Batch 22/38] [D loss: 0.5798928737640381] [G loss: 0.9555627703666687]\n",
      "[Epoch 31/40] [Batch 23/38] [D loss: 0.413349986076355] [G loss: 1.8151696920394897]\n",
      "[Epoch 31/40] [Batch 24/38] [D loss: 0.4249849319458008] [G loss: 1.3233630657196045]\n",
      "[Epoch 31/40] [Batch 25/38] [D loss: 0.5061033964157104] [G loss: 0.9551728963851929]\n",
      "[Epoch 31/40] [Batch 26/38] [D loss: 0.46137160062789917] [G loss: 1.1319854259490967]\n",
      "[Epoch 31/40] [Batch 27/38] [D loss: 0.4544389247894287] [G loss: 1.6022624969482422]\n",
      "[Epoch 31/40] [Batch 28/38] [D loss: 0.42314422130584717] [G loss: 1.16812002658844]\n",
      "[Epoch 31/40] [Batch 29/38] [D loss: 0.5674258470535278] [G loss: 1.9304816722869873]\n",
      "[Epoch 31/40] [Batch 30/38] [D loss: 0.49004659056663513] [G loss: 1.0550765991210938]\n",
      "[Epoch 31/40] [Batch 31/38] [D loss: 0.4553912281990051] [G loss: 0.9811763167381287]\n",
      "[Epoch 31/40] [Batch 32/38] [D loss: 0.5720462799072266] [G loss: 1.5661543607711792]\n",
      "[Epoch 31/40] [Batch 33/38] [D loss: 0.4437446892261505] [G loss: 1.360946774482727]\n",
      "[Epoch 31/40] [Batch 34/38] [D loss: 0.5207823514938354] [G loss: 1.1771365404129028]\n",
      "[Epoch 31/40] [Batch 35/38] [D loss: 0.45309263467788696] [G loss: 1.0264495611190796]\n",
      "[Epoch 31/40] [Batch 36/38] [D loss: 0.40310120582580566] [G loss: 1.8162245750427246]\n",
      "[Epoch 31/40] [Batch 37/38] [D loss: 0.5300101041793823] [G loss: 1.980420708656311]\n",
      "[Epoch 32/40] [Batch 0/38] [D loss: 0.7007841467857361] [G loss: 0.6029642820358276]\n",
      "[Epoch 32/40] [Batch 1/38] [D loss: 0.5207552909851074] [G loss: 1.483620047569275]\n",
      "[Epoch 32/40] [Batch 2/38] [D loss: 0.4072318971157074] [G loss: 2.398911952972412]\n",
      "[Epoch 32/40] [Batch 3/38] [D loss: 0.5464944243431091] [G loss: 0.8516862392425537]\n",
      "[Epoch 32/40] [Batch 4/38] [D loss: 0.4569196105003357] [G loss: 1.2811075448989868]\n",
      "[Epoch 32/40] [Batch 5/38] [D loss: 0.5799285173416138] [G loss: 1.9249415397644043]\n",
      "[Epoch 32/40] [Batch 6/38] [D loss: 0.5417678952217102] [G loss: 1.1538249254226685]\n",
      "[Epoch 32/40] [Batch 7/38] [D loss: 0.6147751212120056] [G loss: 0.7435219883918762]\n",
      "[Epoch 32/40] [Batch 8/38] [D loss: 0.6546537280082703] [G loss: 1.64479660987854]\n",
      "[Epoch 32/40] [Batch 9/38] [D loss: 0.5373444557189941] [G loss: 1.0968742370605469]\n",
      "[Epoch 32/40] [Batch 10/38] [D loss: 0.43229109048843384] [G loss: 1.2808688879013062]\n",
      "[Epoch 32/40] [Batch 11/38] [D loss: 0.4083988070487976] [G loss: 1.6816380023956299]\n",
      "[Epoch 32/40] [Batch 12/38] [D loss: 0.5597914457321167] [G loss: 1.5577949285507202]\n",
      "[Epoch 32/40] [Batch 13/38] [D loss: 0.4865441918373108] [G loss: 1.2081173658370972]\n",
      "[Epoch 32/40] [Batch 14/38] [D loss: 0.6502076983451843] [G loss: 0.7640537619590759]\n",
      "[Epoch 32/40] [Batch 15/38] [D loss: 0.4102521538734436] [G loss: 1.2408686876296997]\n",
      "[Epoch 32/40] [Batch 16/38] [D loss: 0.49137404561042786] [G loss: 1.8487942218780518]\n",
      "[Epoch 32/40] [Batch 17/38] [D loss: 0.467669278383255] [G loss: 1.2059533596038818]\n",
      "[Epoch 32/40] [Batch 18/38] [D loss: 0.5148769617080688] [G loss: 1.0291378498077393]\n",
      "[Epoch 32/40] [Batch 19/38] [D loss: 0.33407658338546753] [G loss: 1.2962803840637207]\n",
      "[Epoch 32/40] [Batch 20/38] [D loss: 0.3288610279560089] [G loss: 1.6920995712280273]\n",
      "[Epoch 32/40] [Batch 21/38] [D loss: 0.25809937715530396] [G loss: 1.701640248298645]\n",
      "[Epoch 32/40] [Batch 22/38] [D loss: 0.2914595901966095] [G loss: 1.345549464225769]\n",
      "[Epoch 32/40] [Batch 23/38] [D loss: 0.46108341217041016] [G loss: 1.3160043954849243]\n",
      "[Epoch 32/40] [Batch 24/38] [D loss: 0.5640960931777954] [G loss: 1.3858832120895386]\n",
      "[Epoch 32/40] [Batch 25/38] [D loss: 0.5480543971061707] [G loss: 0.848660945892334]\n",
      "[Epoch 32/40] [Batch 26/38] [D loss: 0.4940139651298523] [G loss: 1.1355764865875244]\n",
      "[Epoch 32/40] [Batch 27/38] [D loss: 0.3952391743659973] [G loss: 1.933633804321289]\n",
      "[Epoch 32/40] [Batch 28/38] [D loss: 0.6503618955612183] [G loss: 1.4172859191894531]\n",
      "[Epoch 32/40] [Batch 29/38] [D loss: 0.4999997019767761] [G loss: 0.9882742762565613]\n",
      "[Epoch 32/40] [Batch 30/38] [D loss: 0.44523751735687256] [G loss: 1.1486321687698364]\n",
      "[Epoch 32/40] [Batch 31/38] [D loss: 0.518665611743927] [G loss: 1.6049586534500122]\n",
      "[Epoch 32/40] [Batch 32/38] [D loss: 0.3845059871673584] [G loss: 1.408766269683838]\n",
      "[Epoch 32/40] [Batch 33/38] [D loss: 0.44193917512893677] [G loss: 1.0692100524902344]\n",
      "[Epoch 32/40] [Batch 34/38] [D loss: 0.4862593412399292] [G loss: 1.753453254699707]\n",
      "[Epoch 32/40] [Batch 35/38] [D loss: 0.4925472140312195] [G loss: 1.3894524574279785]\n",
      "[Epoch 32/40] [Batch 36/38] [D loss: 0.5288466215133667] [G loss: 1.108756422996521]\n",
      "[Epoch 32/40] [Batch 37/38] [D loss: 1.0411481857299805] [G loss: 0.9420010447502136]\n",
      "[Epoch 33/40] [Batch 0/38] [D loss: 0.6399086713790894] [G loss: 0.4840415120124817]\n",
      "[Epoch 33/40] [Batch 1/38] [D loss: 0.4840549826622009] [G loss: 1.7661106586456299]\n",
      "[Epoch 33/40] [Batch 2/38] [D loss: 0.42496615648269653] [G loss: 1.2941510677337646]\n",
      "[Epoch 33/40] [Batch 3/38] [D loss: 0.5180109739303589] [G loss: 1.1928918361663818]\n",
      "[Epoch 33/40] [Batch 4/38] [D loss: 0.5671372413635254] [G loss: 1.1108384132385254]\n",
      "[Epoch 33/40] [Batch 5/38] [D loss: 0.533764123916626] [G loss: 0.9454824328422546]\n",
      "[Epoch 33/40] [Batch 6/38] [D loss: 0.43531566858291626] [G loss: 1.5803947448730469]\n",
      "[Epoch 33/40] [Batch 7/38] [D loss: 0.49776944518089294] [G loss: 1.3470752239227295]\n",
      "[Epoch 33/40] [Batch 8/38] [D loss: 0.6629471778869629] [G loss: 1.2430886030197144]\n",
      "[Epoch 33/40] [Batch 9/38] [D loss: 0.5433260798454285] [G loss: 1.2549164295196533]\n",
      "[Epoch 33/40] [Batch 10/38] [D loss: 0.4648802578449249] [G loss: 1.5187066793441772]\n",
      "[Epoch 33/40] [Batch 11/38] [D loss: 0.6501296758651733] [G loss: 1.432539939880371]\n",
      "[Epoch 33/40] [Batch 12/38] [D loss: 0.7903242707252502] [G loss: 0.8057653307914734]\n",
      "[Epoch 33/40] [Batch 13/38] [D loss: 0.6640627384185791] [G loss: 0.9691661596298218]\n",
      "[Epoch 33/40] [Batch 14/38] [D loss: 0.6261869072914124] [G loss: 1.6390868425369263]\n",
      "[Epoch 33/40] [Batch 15/38] [D loss: 0.5058126449584961] [G loss: 1.2197109460830688]\n",
      "[Epoch 33/40] [Batch 16/38] [D loss: 0.47018253803253174] [G loss: 1.1054813861846924]\n",
      "[Epoch 33/40] [Batch 17/38] [D loss: 0.49441736936569214] [G loss: 1.314201831817627]\n",
      "[Epoch 33/40] [Batch 18/38] [D loss: 0.4591525197029114] [G loss: 1.359594702720642]\n",
      "[Epoch 33/40] [Batch 19/38] [D loss: 0.5134326219558716] [G loss: 1.292206048965454]\n",
      "[Epoch 33/40] [Batch 20/38] [D loss: 0.4653526246547699] [G loss: 1.9488492012023926]\n",
      "[Epoch 33/40] [Batch 21/38] [D loss: 0.6341145038604736] [G loss: 1.2333093881607056]\n",
      "[Epoch 33/40] [Batch 22/38] [D loss: 0.514317512512207] [G loss: 1.4098562002182007]\n",
      "[Epoch 33/40] [Batch 23/38] [D loss: 0.5326652526855469] [G loss: 1.0070629119873047]\n",
      "[Epoch 33/40] [Batch 24/38] [D loss: 0.48947182297706604] [G loss: 1.1773576736450195]\n",
      "[Epoch 33/40] [Batch 25/38] [D loss: 0.5898373126983643] [G loss: 1.3169476985931396]\n",
      "[Epoch 33/40] [Batch 26/38] [D loss: 0.6215957403182983] [G loss: 0.8361842632293701]\n",
      "[Epoch 33/40] [Batch 27/38] [D loss: 0.5723050832748413] [G loss: 1.23075270652771]\n",
      "[Epoch 33/40] [Batch 28/38] [D loss: 0.5569100379943848] [G loss: 1.3530813455581665]\n",
      "[Epoch 33/40] [Batch 29/38] [D loss: 0.6074511408805847] [G loss: 1.4288995265960693]\n",
      "[Epoch 33/40] [Batch 30/38] [D loss: 0.5427982211112976] [G loss: 0.9289044737815857]\n",
      "[Epoch 33/40] [Batch 31/38] [D loss: 0.6766456365585327] [G loss: 1.421785593032837]\n",
      "[Epoch 33/40] [Batch 32/38] [D loss: 0.49088168144226074] [G loss: 0.8274238705635071]\n",
      "[Epoch 33/40] [Batch 33/38] [D loss: 0.4904494285583496] [G loss: 1.0298292636871338]\n",
      "[Epoch 33/40] [Batch 34/38] [D loss: 0.4828363060951233] [G loss: 1.9233261346817017]\n",
      "[Epoch 33/40] [Batch 35/38] [D loss: 0.5511581301689148] [G loss: 1.6042520999908447]\n",
      "[Epoch 33/40] [Batch 36/38] [D loss: 0.5718324184417725] [G loss: 0.8278253674507141]\n",
      "[Epoch 33/40] [Batch 37/38] [D loss: 0.42953866720199585] [G loss: 1.4469413757324219]\n",
      "[Epoch 34/40] [Batch 0/38] [D loss: 0.47948187589645386] [G loss: 1.2094917297363281]\n",
      "[Epoch 34/40] [Batch 1/38] [D loss: 0.5317721962928772] [G loss: 1.6692910194396973]\n",
      "[Epoch 34/40] [Batch 2/38] [D loss: 0.39115390181541443] [G loss: 0.9709067344665527]\n",
      "[Epoch 34/40] [Batch 3/38] [D loss: 0.501899003982544] [G loss: 1.972931146621704]\n",
      "[Epoch 34/40] [Batch 4/38] [D loss: 0.5533351898193359] [G loss: 1.3447359800338745]\n",
      "[Epoch 34/40] [Batch 5/38] [D loss: 0.42017632722854614] [G loss: 1.5438508987426758]\n",
      "[Epoch 34/40] [Batch 6/38] [D loss: 0.4571534991264343] [G loss: 1.6266248226165771]\n",
      "[Epoch 34/40] [Batch 7/38] [D loss: 0.5754594206809998] [G loss: 1.1608710289001465]\n",
      "[Epoch 34/40] [Batch 8/38] [D loss: 0.4959295392036438] [G loss: 1.3317532539367676]\n",
      "[Epoch 34/40] [Batch 9/38] [D loss: 0.4454580843448639] [G loss: 1.4337029457092285]\n",
      "[Epoch 34/40] [Batch 10/38] [D loss: 0.490594744682312] [G loss: 1.0825626850128174]\n",
      "[Epoch 34/40] [Batch 11/38] [D loss: 0.4413156509399414] [G loss: 1.5452039241790771]\n",
      "[Epoch 34/40] [Batch 12/38] [D loss: 0.48002076148986816] [G loss: 1.2525749206542969]\n",
      "[Epoch 34/40] [Batch 13/38] [D loss: 0.5509252548217773] [G loss: 1.4704145193099976]\n",
      "[Epoch 34/40] [Batch 14/38] [D loss: 0.6912925243377686] [G loss: 1.062652587890625]\n",
      "[Epoch 34/40] [Batch 15/38] [D loss: 0.4875498414039612] [G loss: 1.5932741165161133]\n",
      "[Epoch 34/40] [Batch 16/38] [D loss: 0.37797003984451294] [G loss: 1.6193515062332153]\n",
      "[Epoch 34/40] [Batch 17/38] [D loss: 0.33060774207115173] [G loss: 1.5134512186050415]\n",
      "[Epoch 34/40] [Batch 18/38] [D loss: 0.5669310688972473] [G loss: 2.0321450233459473]\n",
      "[Epoch 34/40] [Batch 19/38] [D loss: 0.55326247215271] [G loss: 1.3510838747024536]\n",
      "[Epoch 34/40] [Batch 20/38] [D loss: 0.5632360577583313] [G loss: 1.023018479347229]\n",
      "[Epoch 34/40] [Batch 21/38] [D loss: 0.4101163148880005] [G loss: 1.4279992580413818]\n",
      "[Epoch 34/40] [Batch 22/38] [D loss: 0.5088298320770264] [G loss: 1.649670124053955]\n",
      "[Epoch 34/40] [Batch 23/38] [D loss: 0.6373021602630615] [G loss: 0.8581154942512512]\n",
      "[Epoch 34/40] [Batch 24/38] [D loss: 0.5383071303367615] [G loss: 1.319699764251709]\n",
      "[Epoch 34/40] [Batch 25/38] [D loss: 0.5021370649337769] [G loss: 1.1497470140457153]\n",
      "[Epoch 34/40] [Batch 26/38] [D loss: 0.519622802734375] [G loss: 1.5235295295715332]\n",
      "[Epoch 34/40] [Batch 27/38] [D loss: 0.36686885356903076] [G loss: 1.555294394493103]\n",
      "[Epoch 34/40] [Batch 28/38] [D loss: 0.3176708221435547] [G loss: 1.2338379621505737]\n",
      "[Epoch 34/40] [Batch 29/38] [D loss: 0.53578120470047] [G loss: 1.5079041719436646]\n",
      "[Epoch 34/40] [Batch 30/38] [D loss: 0.65953528881073] [G loss: 1.25214684009552]\n",
      "[Epoch 34/40] [Batch 31/38] [D loss: 0.6684333682060242] [G loss: 0.8679546117782593]\n",
      "[Epoch 34/40] [Batch 32/38] [D loss: 0.45627307891845703] [G loss: 1.728710412979126]\n",
      "[Epoch 34/40] [Batch 33/38] [D loss: 0.48389190435409546] [G loss: 1.2087507247924805]\n",
      "[Epoch 34/40] [Batch 34/38] [D loss: 0.4489501714706421] [G loss: 1.542898416519165]\n",
      "[Epoch 34/40] [Batch 35/38] [D loss: 0.49241894483566284] [G loss: 1.230806589126587]\n",
      "[Epoch 34/40] [Batch 36/38] [D loss: 0.4142211079597473] [G loss: 1.0296417474746704]\n",
      "[Epoch 34/40] [Batch 37/38] [D loss: 0.5291092991828918] [G loss: 1.9517252445220947]\n",
      "[Epoch 35/40] [Batch 0/38] [D loss: 0.5378533601760864] [G loss: 1.0226778984069824]\n",
      "[Epoch 35/40] [Batch 1/38] [D loss: 0.35285064578056335] [G loss: 1.5144751071929932]\n",
      "[Epoch 35/40] [Batch 2/38] [D loss: 0.4094832241535187] [G loss: 1.3556663990020752]\n",
      "[Epoch 35/40] [Batch 3/38] [D loss: 0.7131744623184204] [G loss: 1.9934974908828735]\n",
      "[Epoch 35/40] [Batch 4/38] [D loss: 0.5293720960617065] [G loss: 1.0665760040283203]\n",
      "[Epoch 35/40] [Batch 5/38] [D loss: 0.5648654699325562] [G loss: 0.7930609583854675]\n",
      "[Epoch 35/40] [Batch 6/38] [D loss: 0.40198811888694763] [G loss: 2.192636489868164]\n",
      "[Epoch 35/40] [Batch 7/38] [D loss: 0.44991186261177063] [G loss: 1.2876602411270142]\n",
      "[Epoch 35/40] [Batch 8/38] [D loss: 0.7070010900497437] [G loss: 1.0931004285812378]\n",
      "[Epoch 35/40] [Batch 9/38] [D loss: 0.5857177972793579] [G loss: 0.6896179914474487]\n",
      "[Epoch 35/40] [Batch 10/38] [D loss: 0.4766218066215515] [G loss: 1.9470453262329102]\n",
      "[Epoch 35/40] [Batch 11/38] [D loss: 0.40406349301338196] [G loss: 1.3651304244995117]\n",
      "[Epoch 35/40] [Batch 12/38] [D loss: 0.4613453149795532] [G loss: 0.992678165435791]\n",
      "[Epoch 35/40] [Batch 13/38] [D loss: 0.7156494855880737] [G loss: 1.083986520767212]\n",
      "[Epoch 35/40] [Batch 14/38] [D loss: 0.5580329298973083] [G loss: 1.0329210758209229]\n",
      "[Epoch 35/40] [Batch 15/38] [D loss: 0.43737202882766724] [G loss: 1.3224403858184814]\n",
      "[Epoch 35/40] [Batch 16/38] [D loss: 0.5509344339370728] [G loss: 1.751229166984558]\n",
      "[Epoch 35/40] [Batch 17/38] [D loss: 0.5902301073074341] [G loss: 1.4704946279525757]\n",
      "[Epoch 35/40] [Batch 18/38] [D loss: 0.4196569323539734] [G loss: 0.966891884803772]\n",
      "[Epoch 35/40] [Batch 19/38] [D loss: 0.4626106023788452] [G loss: 1.2239893674850464]\n",
      "[Epoch 35/40] [Batch 20/38] [D loss: 0.5103179216384888] [G loss: 1.7878050804138184]\n",
      "[Epoch 35/40] [Batch 21/38] [D loss: 0.36066240072250366] [G loss: 1.8563740253448486]\n",
      "[Epoch 35/40] [Batch 22/38] [D loss: 0.44924765825271606] [G loss: 0.94484543800354]\n",
      "[Epoch 35/40] [Batch 23/38] [D loss: 0.4545213580131531] [G loss: 1.5639132261276245]\n",
      "[Epoch 35/40] [Batch 24/38] [D loss: 0.5119994878768921] [G loss: 1.4876000881195068]\n",
      "[Epoch 35/40] [Batch 25/38] [D loss: 0.6158698797225952] [G loss: 1.0375242233276367]\n",
      "[Epoch 35/40] [Batch 26/38] [D loss: 0.5987740755081177] [G loss: 1.9918135404586792]\n",
      "[Epoch 35/40] [Batch 27/38] [D loss: 0.4246083199977875] [G loss: 1.587827444076538]\n",
      "[Epoch 35/40] [Batch 28/38] [D loss: 0.4911075830459595] [G loss: 1.3090894222259521]\n",
      "[Epoch 35/40] [Batch 29/38] [D loss: 0.3901808261871338] [G loss: 1.1911957263946533]\n",
      "[Epoch 35/40] [Batch 30/38] [D loss: 0.4438599944114685] [G loss: 1.6704213619232178]\n",
      "[Epoch 35/40] [Batch 31/38] [D loss: 0.5677905082702637] [G loss: 1.8337855339050293]\n",
      "[Epoch 35/40] [Batch 32/38] [D loss: 0.6662033796310425] [G loss: 0.8203701972961426]\n",
      "[Epoch 35/40] [Batch 33/38] [D loss: 0.6358041763305664] [G loss: 1.674769639968872]\n",
      "[Epoch 35/40] [Batch 34/38] [D loss: 0.46933871507644653] [G loss: 1.2237155437469482]\n",
      "[Epoch 35/40] [Batch 35/38] [D loss: 0.4179571270942688] [G loss: 1.613750696182251]\n",
      "[Epoch 35/40] [Batch 36/38] [D loss: 0.35575631260871887] [G loss: 1.6933932304382324]\n",
      "[Epoch 35/40] [Batch 37/38] [D loss: 0.48569220304489136] [G loss: 1.1575573682785034]\n",
      "[Epoch 36/40] [Batch 0/38] [D loss: 0.4913216531276703] [G loss: 1.0254876613616943]\n",
      "[Epoch 36/40] [Batch 1/38] [D loss: 0.47962045669555664] [G loss: 1.4396711587905884]\n",
      "[Epoch 36/40] [Batch 2/38] [D loss: 0.3520805239677429] [G loss: 1.6180230379104614]\n",
      "[Epoch 36/40] [Batch 3/38] [D loss: 0.5298430919647217] [G loss: 0.9542847871780396]\n",
      "[Epoch 36/40] [Batch 4/38] [D loss: 0.43778175115585327] [G loss: 1.856323480606079]\n",
      "[Epoch 36/40] [Batch 5/38] [D loss: 0.3790479898452759] [G loss: 1.1816140413284302]\n",
      "[Epoch 36/40] [Batch 6/38] [D loss: 0.5943936109542847] [G loss: 1.7128077745437622]\n",
      "[Epoch 36/40] [Batch 7/38] [D loss: 0.41805440187454224] [G loss: 1.3527354001998901]\n",
      "[Epoch 36/40] [Batch 8/38] [D loss: 0.4273180365562439] [G loss: 1.4370614290237427]\n",
      "[Epoch 36/40] [Batch 9/38] [D loss: 0.5809574723243713] [G loss: 2.113229751586914]\n",
      "[Epoch 36/40] [Batch 10/38] [D loss: 0.6405308842658997] [G loss: 0.7599307298660278]\n",
      "[Epoch 36/40] [Batch 11/38] [D loss: 0.3861309289932251] [G loss: 1.3451120853424072]\n",
      "[Epoch 36/40] [Batch 12/38] [D loss: 0.4643458127975464] [G loss: 1.9345670938491821]\n",
      "[Epoch 36/40] [Batch 13/38] [D loss: 0.6439918279647827] [G loss: 0.7278435826301575]\n",
      "[Epoch 36/40] [Batch 14/38] [D loss: 0.46589940786361694] [G loss: 1.3169382810592651]\n",
      "[Epoch 36/40] [Batch 15/38] [D loss: 0.5476069450378418] [G loss: 1.8794103860855103]\n",
      "[Epoch 36/40] [Batch 16/38] [D loss: 0.44875568151474] [G loss: 1.2477068901062012]\n",
      "[Epoch 36/40] [Batch 17/38] [D loss: 0.5441203117370605] [G loss: 1.5249176025390625]\n",
      "[Epoch 36/40] [Batch 18/38] [D loss: 0.40668797492980957] [G loss: 1.287349820137024]\n",
      "[Epoch 36/40] [Batch 19/38] [D loss: 0.29526716470718384] [G loss: 1.5474979877471924]\n",
      "[Epoch 36/40] [Batch 20/38] [D loss: 0.5280119180679321] [G loss: 1.1605942249298096]\n",
      "[Epoch 36/40] [Batch 21/38] [D loss: 0.6183212995529175] [G loss: 1.2383196353912354]\n",
      "[Epoch 36/40] [Batch 22/38] [D loss: 0.5741791725158691] [G loss: 1.2561566829681396]\n",
      "[Epoch 36/40] [Batch 23/38] [D loss: 0.5944961309432983] [G loss: 1.081547498703003]\n",
      "[Epoch 36/40] [Batch 24/38] [D loss: 0.6336192488670349] [G loss: 1.2940670251846313]\n",
      "[Epoch 36/40] [Batch 25/38] [D loss: 0.5631736516952515] [G loss: 1.013365626335144]\n",
      "[Epoch 36/40] [Batch 26/38] [D loss: 0.5300549268722534] [G loss: 1.773532748222351]\n",
      "[Epoch 36/40] [Batch 27/38] [D loss: 0.5794409513473511] [G loss: 1.030192494392395]\n",
      "[Epoch 36/40] [Batch 28/38] [D loss: 0.5475090742111206] [G loss: 1.3321359157562256]\n",
      "[Epoch 36/40] [Batch 29/38] [D loss: 0.41404902935028076] [G loss: 1.3725461959838867]\n",
      "[Epoch 36/40] [Batch 30/38] [D loss: 0.408438503742218] [G loss: 1.365025281906128]\n",
      "[Epoch 36/40] [Batch 31/38] [D loss: 0.46545833349227905] [G loss: 1.291441798210144]\n",
      "[Epoch 36/40] [Batch 32/38] [D loss: 0.3912287950515747] [G loss: 1.526856780052185]\n",
      "[Epoch 36/40] [Batch 33/38] [D loss: 0.5233094692230225] [G loss: 1.213804841041565]\n",
      "[Epoch 36/40] [Batch 34/38] [D loss: 0.577967643737793] [G loss: 1.0741947889328003]\n",
      "[Epoch 36/40] [Batch 35/38] [D loss: 0.5012575387954712] [G loss: 1.5200042724609375]\n",
      "[Epoch 36/40] [Batch 36/38] [D loss: 0.6504544019699097] [G loss: 1.327893614768982]\n",
      "[Epoch 36/40] [Batch 37/38] [D loss: 0.46251600980758667] [G loss: 0.8174786567687988]\n",
      "[Epoch 37/40] [Batch 0/38] [D loss: 0.5413267612457275] [G loss: 1.040248990058899]\n",
      "[Epoch 37/40] [Batch 1/38] [D loss: 0.3979164958000183] [G loss: 1.9220424890518188]\n",
      "[Epoch 37/40] [Batch 2/38] [D loss: 0.3038546144962311] [G loss: 1.7201908826828003]\n",
      "[Epoch 37/40] [Batch 3/38] [D loss: 0.41897255182266235] [G loss: 1.1907199621200562]\n",
      "[Epoch 37/40] [Batch 4/38] [D loss: 0.4148257374763489] [G loss: 1.5041179656982422]\n",
      "[Epoch 37/40] [Batch 5/38] [D loss: 0.583578884601593] [G loss: 1.300811529159546]\n",
      "[Epoch 37/40] [Batch 6/38] [D loss: 0.7775030136108398] [G loss: 0.5081408023834229]\n",
      "[Epoch 37/40] [Batch 7/38] [D loss: 0.4751855731010437] [G loss: 1.2614364624023438]\n",
      "[Epoch 37/40] [Batch 8/38] [D loss: 0.5309748649597168] [G loss: 1.6213295459747314]\n",
      "[Epoch 37/40] [Batch 9/38] [D loss: 0.5696741342544556] [G loss: 1.6721582412719727]\n",
      "[Epoch 37/40] [Batch 10/38] [D loss: 0.688900887966156] [G loss: 0.6227777004241943]\n",
      "[Epoch 37/40] [Batch 11/38] [D loss: 0.41550102829933167] [G loss: 1.8396108150482178]\n",
      "[Epoch 37/40] [Batch 12/38] [D loss: 0.3551273047924042] [G loss: 1.5955990552902222]\n",
      "[Epoch 37/40] [Batch 13/38] [D loss: 0.5470772981643677] [G loss: 1.2129435539245605]\n",
      "[Epoch 37/40] [Batch 14/38] [D loss: 0.3808845579624176] [G loss: 1.6821351051330566]\n",
      "[Epoch 37/40] [Batch 15/38] [D loss: 0.49363628029823303] [G loss: 1.5891656875610352]\n",
      "[Epoch 37/40] [Batch 16/38] [D loss: 0.6354482173919678] [G loss: 1.1718426942825317]\n",
      "[Epoch 37/40] [Batch 17/38] [D loss: 0.4768253564834595] [G loss: 1.467611312866211]\n",
      "[Epoch 37/40] [Batch 18/38] [D loss: 0.531097412109375] [G loss: 1.0989655256271362]\n",
      "[Epoch 37/40] [Batch 19/38] [D loss: 0.3506908416748047] [G loss: 1.1201058626174927]\n",
      "[Epoch 37/40] [Batch 20/38] [D loss: 0.5451046228408813] [G loss: 2.0187339782714844]\n",
      "[Epoch 37/40] [Batch 21/38] [D loss: 0.46388736367225647] [G loss: 1.3015269041061401]\n",
      "[Epoch 37/40] [Batch 22/38] [D loss: 0.35735780000686646] [G loss: 1.3676832914352417]\n",
      "[Epoch 37/40] [Batch 23/38] [D loss: 0.491741418838501] [G loss: 2.0190978050231934]\n",
      "[Epoch 37/40] [Batch 24/38] [D loss: 0.6366771459579468] [G loss: 0.8869554996490479]\n",
      "[Epoch 37/40] [Batch 25/38] [D loss: 0.5709538459777832] [G loss: 1.4720966815948486]\n",
      "[Epoch 37/40] [Batch 26/38] [D loss: 0.5474835634231567] [G loss: 0.8917372822761536]\n",
      "[Epoch 37/40] [Batch 27/38] [D loss: 0.6965548396110535] [G loss: 1.8705134391784668]\n",
      "[Epoch 37/40] [Batch 28/38] [D loss: 0.5304763317108154] [G loss: 1.0408565998077393]\n",
      "[Epoch 37/40] [Batch 29/38] [D loss: 0.42029064893722534] [G loss: 1.3260862827301025]\n",
      "[Epoch 37/40] [Batch 30/38] [D loss: 0.5836754441261292] [G loss: 1.5594871044158936]\n",
      "[Epoch 37/40] [Batch 31/38] [D loss: 0.4790154993534088] [G loss: 1.2552458047866821]\n",
      "[Epoch 37/40] [Batch 32/38] [D loss: 0.47618669271469116] [G loss: 1.3820490837097168]\n",
      "[Epoch 37/40] [Batch 33/38] [D loss: 0.45488786697387695] [G loss: 0.9889718294143677]\n",
      "[Epoch 37/40] [Batch 34/38] [D loss: 0.5627363324165344] [G loss: 1.1601200103759766]\n",
      "[Epoch 37/40] [Batch 35/38] [D loss: 0.49695122241973877] [G loss: 1.3412302732467651]\n",
      "[Epoch 37/40] [Batch 36/38] [D loss: 0.4130716025829315] [G loss: 1.6299511194229126]\n",
      "[Epoch 37/40] [Batch 37/38] [D loss: 0.6130407452583313] [G loss: 0.7603875398635864]\n",
      "[Epoch 38/40] [Batch 0/38] [D loss: 0.584997296333313] [G loss: 1.5620625019073486]\n",
      "[Epoch 38/40] [Batch 1/38] [D loss: 0.4607897400856018] [G loss: 1.4523433446884155]\n",
      "[Epoch 38/40] [Batch 2/38] [D loss: 0.31263014674186707] [G loss: 1.736374855041504]\n",
      "[Epoch 38/40] [Batch 3/38] [D loss: 0.45320022106170654] [G loss: 1.4988902807235718]\n",
      "[Epoch 38/40] [Batch 4/38] [D loss: 0.6209076642990112] [G loss: 0.9842979907989502]\n",
      "[Epoch 38/40] [Batch 5/38] [D loss: 0.5063729882240295] [G loss: 0.9959246516227722]\n",
      "[Epoch 38/40] [Batch 6/38] [D loss: 0.604089617729187] [G loss: 1.8460856676101685]\n",
      "[Epoch 38/40] [Batch 7/38] [D loss: 0.5387187600135803] [G loss: 1.1129419803619385]\n",
      "[Epoch 38/40] [Batch 8/38] [D loss: 0.5693552494049072] [G loss: 1.6680372953414917]\n",
      "[Epoch 38/40] [Batch 9/38] [D loss: 0.42656588554382324] [G loss: 1.0208805799484253]\n",
      "[Epoch 38/40] [Batch 10/38] [D loss: 0.44403940439224243] [G loss: 1.742821455001831]\n",
      "[Epoch 38/40] [Batch 11/38] [D loss: 0.41846323013305664] [G loss: 1.3691117763519287]\n",
      "[Epoch 38/40] [Batch 12/38] [D loss: 0.6523783206939697] [G loss: 1.183035135269165]\n",
      "[Epoch 38/40] [Batch 13/38] [D loss: 0.6477174162864685] [G loss: 1.0934433937072754]\n",
      "[Epoch 38/40] [Batch 14/38] [D loss: 0.5757798552513123] [G loss: 0.9243922233581543]\n",
      "[Epoch 38/40] [Batch 15/38] [D loss: 0.6209237575531006] [G loss: 1.135491132736206]\n",
      "[Epoch 38/40] [Batch 16/38] [D loss: 0.38411104679107666] [G loss: 1.4596439599990845]\n",
      "[Epoch 38/40] [Batch 17/38] [D loss: 0.5053541660308838] [G loss: 1.452725887298584]\n",
      "[Epoch 38/40] [Batch 18/38] [D loss: 0.5549315810203552] [G loss: 1.1209908723831177]\n",
      "[Epoch 38/40] [Batch 19/38] [D loss: 0.5735802054405212] [G loss: 1.1643332242965698]\n",
      "[Epoch 38/40] [Batch 20/38] [D loss: 0.5533099174499512] [G loss: 1.7043578624725342]\n",
      "[Epoch 38/40] [Batch 21/38] [D loss: 0.491134375333786] [G loss: 1.2520020008087158]\n",
      "[Epoch 38/40] [Batch 22/38] [D loss: 0.4250066578388214] [G loss: 0.9205930233001709]\n",
      "[Epoch 38/40] [Batch 23/38] [D loss: 0.3963613510131836] [G loss: 1.3581068515777588]\n",
      "[Epoch 38/40] [Batch 24/38] [D loss: 0.5755444169044495] [G loss: 1.2014405727386475]\n",
      "[Epoch 38/40] [Batch 25/38] [D loss: 0.46248018741607666] [G loss: 1.279697299003601]\n",
      "[Epoch 38/40] [Batch 26/38] [D loss: 0.407378613948822] [G loss: 1.477276086807251]\n",
      "[Epoch 38/40] [Batch 27/38] [D loss: 0.5331451296806335] [G loss: 1.0040972232818604]\n",
      "[Epoch 38/40] [Batch 28/38] [D loss: 0.4286900758743286] [G loss: 1.062084436416626]\n",
      "[Epoch 38/40] [Batch 29/38] [D loss: 0.3955215811729431] [G loss: 1.0795392990112305]\n",
      "[Epoch 38/40] [Batch 30/38] [D loss: 0.37110650539398193] [G loss: 1.7603107690811157]\n",
      "[Epoch 38/40] [Batch 31/38] [D loss: 0.5239250659942627] [G loss: 1.687021255493164]\n",
      "[Epoch 38/40] [Batch 32/38] [D loss: 0.5754892230033875] [G loss: 1.1048847436904907]\n",
      "[Epoch 38/40] [Batch 33/38] [D loss: 0.5913959741592407] [G loss: 1.6194560527801514]\n",
      "[Epoch 38/40] [Batch 34/38] [D loss: 0.4442359209060669] [G loss: 1.2424384355545044]\n",
      "[Epoch 38/40] [Batch 35/38] [D loss: 0.5697958469390869] [G loss: 1.349717140197754]\n",
      "[Epoch 38/40] [Batch 36/38] [D loss: 0.5288704037666321] [G loss: 1.0697253942489624]\n",
      "[Epoch 38/40] [Batch 37/38] [D loss: 0.4568735361099243] [G loss: 2.3590354919433594]\n",
      "[Epoch 39/40] [Batch 0/38] [D loss: 0.42223936319351196] [G loss: 0.89885413646698]\n",
      "[Epoch 39/40] [Batch 1/38] [D loss: 0.3222982883453369] [G loss: 1.3876477479934692]\n",
      "[Epoch 39/40] [Batch 2/38] [D loss: 0.4448912739753723] [G loss: 1.5510368347167969]\n",
      "[Epoch 39/40] [Batch 3/38] [D loss: 0.5218607187271118] [G loss: 1.4849607944488525]\n",
      "[Epoch 39/40] [Batch 4/38] [D loss: 0.47886016964912415] [G loss: 1.3017387390136719]\n",
      "[Epoch 39/40] [Batch 5/38] [D loss: 0.5856624245643616] [G loss: 1.3558460474014282]\n",
      "[Epoch 39/40] [Batch 6/38] [D loss: 0.4016578793525696] [G loss: 1.840713381767273]\n",
      "[Epoch 39/40] [Batch 7/38] [D loss: 0.4881576895713806] [G loss: 1.2100162506103516]\n",
      "[Epoch 39/40] [Batch 8/38] [D loss: 0.48677217960357666] [G loss: 1.4452176094055176]\n",
      "[Epoch 39/40] [Batch 9/38] [D loss: 0.29629772901535034] [G loss: 1.703001618385315]\n",
      "[Epoch 39/40] [Batch 10/38] [D loss: 0.31605732440948486] [G loss: 1.5025231838226318]\n",
      "[Epoch 39/40] [Batch 11/38] [D loss: 0.41158509254455566] [G loss: 2.064345121383667]\n",
      "[Epoch 39/40] [Batch 12/38] [D loss: 0.3772636353969574] [G loss: 0.9985583424568176]\n",
      "[Epoch 39/40] [Batch 13/38] [D loss: 0.3458919823169708] [G loss: 1.4799308776855469]\n",
      "[Epoch 39/40] [Batch 14/38] [D loss: 0.3744227886199951] [G loss: 1.8889083862304688]\n",
      "[Epoch 39/40] [Batch 15/38] [D loss: 0.3537593483924866] [G loss: 1.5537878274917603]\n",
      "[Epoch 39/40] [Batch 16/38] [D loss: 0.43438488245010376] [G loss: 0.9913249015808105]\n",
      "[Epoch 39/40] [Batch 17/38] [D loss: 0.42952606081962585] [G loss: 1.2861151695251465]\n",
      "[Epoch 39/40] [Batch 18/38] [D loss: 0.3416762948036194] [G loss: 1.5887641906738281]\n",
      "[Epoch 39/40] [Batch 19/38] [D loss: 0.5635013580322266] [G loss: 0.9482969045639038]\n",
      "[Epoch 39/40] [Batch 20/38] [D loss: 0.38937294483184814] [G loss: 1.706539511680603]\n",
      "[Epoch 39/40] [Batch 21/38] [D loss: 0.5207232236862183] [G loss: 1.7415988445281982]\n",
      "[Epoch 39/40] [Batch 22/38] [D loss: 0.47441691160202026] [G loss: 1.1925870180130005]\n",
      "[Epoch 39/40] [Batch 23/38] [D loss: 0.4580250382423401] [G loss: 2.0084481239318848]\n",
      "[Epoch 39/40] [Batch 24/38] [D loss: 0.47160103917121887] [G loss: 1.023267149925232]\n",
      "[Epoch 39/40] [Batch 25/38] [D loss: 0.426578164100647] [G loss: 1.6912004947662354]\n",
      "[Epoch 39/40] [Batch 26/38] [D loss: 0.5887102484703064] [G loss: 1.7203694581985474]\n",
      "[Epoch 39/40] [Batch 27/38] [D loss: 0.47496363520622253] [G loss: 0.8417189121246338]\n",
      "[Epoch 39/40] [Batch 28/38] [D loss: 0.5251426100730896] [G loss: 2.5393800735473633]\n",
      "[Epoch 39/40] [Batch 29/38] [D loss: 0.4617876410484314] [G loss: 1.4489243030548096]\n",
      "[Epoch 39/40] [Batch 30/38] [D loss: 0.5474517345428467] [G loss: 0.614296019077301]\n",
      "[Epoch 39/40] [Batch 31/38] [D loss: 0.4273257851600647] [G loss: 1.484161376953125]\n",
      "[Epoch 39/40] [Batch 32/38] [D loss: 0.4023023843765259] [G loss: 1.7787286043167114]\n",
      "[Epoch 39/40] [Batch 33/38] [D loss: 0.3813193738460541] [G loss: 1.2403887510299683]\n",
      "[Epoch 39/40] [Batch 34/38] [D loss: 0.33726418018341064] [G loss: 1.878105878829956]\n",
      "[Epoch 39/40] [Batch 35/38] [D loss: 0.37806329131126404] [G loss: 1.8928461074829102]\n",
      "[Epoch 39/40] [Batch 36/38] [D loss: 0.41264116764068604] [G loss: 0.9669705033302307]\n",
      "[Epoch 39/40] [Batch 37/38] [D loss: 0.5745494365692139] [G loss: 1.1674784421920776]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        real_images = imgs.to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(real_images.size(0), 1, device=device)\n",
    "        fake = torch.zeros(real_images.size(0), 1, device=device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(real_images.size(0), latent_dim, device=device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_images = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_images), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_loss = adversarial_loss(discriminator(real_images), valid)\n",
    "        # Loss for fake images\n",
    "        fake_loss = adversarial_loss(discriminator(gen_images.detach()), fake)\n",
    "        # Total discriminator loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Print losses\n",
    "        print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "    # Save generated images\n",
    "    save_generated_images(epoch, generator, latent_dim, output_dir)\n",
    "    # Save the generator weights\n",
    "torch.save(generator.state_dict(), os.path.join(output_dir, 'generator_final.pth'))\n",
    "\n",
    "# Save the discriminator weights\n",
    "torch.save(discriminator.state_dict(), os.path.join(output_dir, 'discriminator_final.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated images saved to 'D:\\GAN\\Newthird\\generated_images2.png'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# Assume the Generator and Discriminator classes have been defined somewhere\n",
    "\n",
    "# Initialize the models\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Load the weights from files\n",
    "generator.load_state_dict(torch.load(r\"D:\\GAN\\Newthird\\generator_final.pth\"))\n",
    "discriminator.load_state_dict(torch.load(r\"D:\\GAN\\Newthird\\discriminator_final.pth\"))\n",
    "\n",
    "# Set to evaluation mode\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "# Specify the directory for saving images\n",
    "output_dir = r\"D:\\GAN\\Newthird\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():  # Important for inference to disable gradient calculation\n",
    "    z = torch.randn(4, latent_dim, device=device)  # 16 is an example batch size for generating 16 images\n",
    "    generated_images = generator(z)\n",
    "    image_path = os.path.join(output_dir, 'generated_images2.png')\n",
    "    save_image(generated_images, image_path, nrow=4, normalize=True)\n",
    "\n",
    "print(f\"Generated images saved to '{image_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet\\diffusion_pytorch_model.safetensors not found\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"duongna/ldm-super-resolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet\\diffusion_pytorch_model.safetensors not found\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "generator.eval()\n",
    "\n",
    "# Load the super-resolution model from Hugging Face\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"duongna/ldm-super-resolution\")\n",
    "pipeline = pipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "generative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
